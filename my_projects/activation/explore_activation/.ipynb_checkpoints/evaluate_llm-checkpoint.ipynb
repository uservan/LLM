{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86f57c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "torch.set_grad_enabled(False)\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.trace_utils import TraceDict2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from counterfact import CounterFactDataset\n",
    "import torch\n",
    "from utils.evaluation_lm_eval import run_eval_harness\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37bed85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gpt_model_and_tokenizer(model_name: str, device='cuda', low_cpu_mem_usage=False):\n",
    "    \"\"\"\n",
    "    Loads a huggingface model and its tokenizer\n",
    "\n",
    "    Parameters:\n",
    "    model_name: huggingface name of the model to load (e.g. GPTJ: \"EleutherAI/gpt-j-6B\", or \"EleutherAI/gpt-j-6b\")\n",
    "    device: 'cuda' or 'cpu'\n",
    "\n",
    "    Returns:\n",
    "    model: huggingface model\n",
    "    tokenizer: huggingface tokenizer\n",
    "    MODEL_CONFIG: config variables w/ standardized names\n",
    "\n",
    "    \"\"\"\n",
    "    assert model_name is not None\n",
    "\n",
    "    print(\"Loading: \", model_name)\n",
    "\n",
    "    if model_name == 'gpt2-xl':\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=low_cpu_mem_usage).to(device)\n",
    "\n",
    "        MODEL_CONFIG = {\"n_heads\": model.config.n_head,\n",
    "                        \"n_layers\": model.config.n_layer,\n",
    "                        \"resid_dim\": model.config.n_embd,\n",
    "                        \"name_or_path\": model.config.name_or_path,\n",
    "                        \"attn_hook_names\": [f'transformer.h.{layer}.attn.c_proj' for layer in\n",
    "                                            range(model.config.n_layer)],\n",
    "                        \"layer_hook_names\": [f'transformer.h.{layer}' for layer in range(model.config.n_layer)]\n",
    "                        }\n",
    "\n",
    "    elif 'gpt-j' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=low_cpu_mem_usage).to(device)\n",
    "\n",
    "        layer_hook_names = [f'transformer.h.{layer}.attn.attn_dropout' for layer in range(model.config.n_layer)]\n",
    "        # layer_hook_names.append('transformer.wte')\n",
    "        # layer_hook_names.append('lm_head')\n",
    "        MODEL_CONFIG = {\"n_heads\": model.config.n_head,\n",
    "                        \"n_layers\": model.config.n_layer,\n",
    "                        \"resid_dim\": model.config.n_embd,\n",
    "                        \"name_or_path\": model.config.name_or_path,\n",
    "                        \"attn_hook_names\": [f'transformer.h.{layer}.attn.out_proj' for layer in\n",
    "                                            range(model.config.n_layer)],\n",
    "                        \"layer_hook_names\": layer_hook_names\n",
    "                        }\n",
    "\n",
    "    elif 'gpt-neox' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "        MODEL_CONFIG = {\"n_heads\": model.config.num_attention_heads,\n",
    "                        \"n_layers\": model.config.num_hidden_layers,\n",
    "                        \"resid_dim\": model.config.hidden_size,\n",
    "                        \"name_or_path\": model.config.name_or_path,\n",
    "                        \"attn_hook_names\": [f'gpt_neox.layers.{layer}.attention.dense' for layer in\n",
    "                                            range(model.config.num_hidden_layers)],\n",
    "                        \"layer_hook_names\": [f'gpt_neox.layers.{layer}' for layer in\n",
    "                                             range(model.config.num_hidden_layers)]}\n",
    "\n",
    "    elif 'llama' in model_name.lower():\n",
    "        if '70b' in model_name.lower():\n",
    "            # use quantization. requires `bitsandbytes` library\n",
    "            from transformers import BitsAndBytesConfig\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type='nf4',\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16\n",
    "            )\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "            model = LlamaForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True,\n",
    "                quantization_config=bnb_config\n",
    "            )\n",
    "        else:\n",
    "            if '7b' in model_name.lower():\n",
    "                model_dtype = torch.float32\n",
    "            else:  # half precision for bigger llama models\n",
    "                model_dtype = torch.float16\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "            model = LlamaForCausalLM.from_pretrained(model_name, torch_dtype=model_dtype).to(device)\n",
    "\n",
    "        MODEL_CONFIG = {\"n_heads\": model.config.num_attention_heads,\n",
    "                        \"n_layers\": model.config.num_hidden_layers,\n",
    "                        \"resid_dim\": model.config.hidden_size,\n",
    "                        \"name_or_path\": model.config._name_or_path,\n",
    "                        \"attn_hook_names\": [f'model.layers.{layer}.self_attn.o_proj' for layer in\n",
    "                                            range(model.config.num_hidden_layers)],\n",
    "                        \"layer_hook_names\": [f'model.layers.{layer}' for layer in\n",
    "                                             range(model.config.num_hidden_layers)]}\n",
    "    else:\n",
    "        raise NotImplementedError(\"Still working to get this model available!\")\n",
    "\n",
    "    return model, tokenizer, MODEL_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca7007b",
   "metadata": {},
   "source": [
    "### gpt-j：normal vs 4-26 zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14abe9f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:  EleutherAI/gpt-j-6b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(4, 26)\n",
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 71026/71026 [51:27<00:00, 23.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 71026/71026 [51:52<00:00, 22.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'results': {'anli_r1': {'acc': 0.324, 'acc_stderr': 0.014806864733738857}, 'anli_r2': {'acc': 0.34, 'acc_stderr': 0.014987482264363937}, 'anli_r3': {'acc': 0.355, 'acc_stderr': 0.013819249004047301}, 'hellaswag': {'acc': 0.4953196574387572, 'acc_stderr': 0.004989562798280521, 'acc_norm': 0.6625174268074089, 'acc_norm_stderr': 0.004718846448021787}, 'piqa': {'acc': 0.7540805223068553, 'acc_stderr': 0.010047331865625191, 'acc_norm': 0.7616974972796517, 'acc_norm_stderr': 0.009940334245876224}, 'winogrande': {'acc': 0.6408839779005525, 'acc_stderr': 0.013483115202120225}, 'wsc': {'acc': 0.36538461538461536, 'acc_stderr': 0.0474473339327792}, 'mathqa': {'acc': 0.26666666666666666, 'acc_stderr': 0.008095350740048926, 'acc_norm': 0.27035175879396983, 'acc_norm_stderr': 0.00813058810331849}}, 'versions': {'anli_r1': 0, 'anli_r2': 0, 'anli_r3': 0, 'hellaswag': 0, 'piqa': 0, 'winogrande': 0, 'wsc': 0, 'mathqa': 0}, 'config': {'name': 'normal_gpt_j'}}\n",
      "{'results': {'anli_r1': {'acc': 0.333, 'acc_stderr': 0.014910846164229871}, 'anli_r2': {'acc': 0.333, 'acc_stderr': 0.014910846164229857}, 'anli_r3': {'acc': 0.3308333333333333, 'acc_stderr': 0.013588208070708999}, 'hellaswag': {'acc': 0.44054969129655447, 'acc_stderr': 0.004954384702021658, 'acc_norm': 0.5916152160924119, 'acc_norm_stderr': 0.004905304371090884}, 'piqa': {'acc': 0.73449401523395, 'acc_stderr': 0.010303308653024427, 'acc_norm': 0.7290533188248096, 'acc_norm_stderr': 0.010369718937426844}, 'winogrande': {'acc': 0.6353591160220995, 'acc_stderr': 0.01352774662242984}, 'wsc': {'acc': 0.36538461538461536, 'acc_stderr': 0.0474473339327792}, 'mathqa': {'acc': 0.2525963149078727, 'acc_stderr': 0.0079541122072996, 'acc_norm': 0.25025125628140704, 'acc_norm_stderr': 0.007929514491487091}}, 'versions': {'anli_r1': 0, 'anli_r2': 0, 'anli_r3': 0, 'hellaswag': 0, 'piqa': 0, 'winogrande': 0, 'wsc': 0, 'mathqa': 0}, 'config': {'name': 'test_gpt_j'}}\n"
     ]
    }
   ],
   "source": [
    "def modify_1(token_id, layer_ids):\n",
    "    def modify_output(output, layer_name, inputs):\n",
    "        # current_layer = int(layer_name.split(\".\")[2])\n",
    "        # if current_layer == edit_layer:\n",
    "        #     if isinstance(output, tuple):\n",
    "        #         output[0][:, idx] += fv_vector.to(device)\n",
    "        #         return output\n",
    "        #     else:\n",
    "        #         return output\n",
    "        # else:\n",
    "        #     return output\n",
    "        return output\n",
    "\n",
    "    def modify_input(input, layer_name:str):\n",
    "        if layer_name.find('wte') != -1:\n",
    "            pass\n",
    "            #print(layer_name)\n",
    "        elif layer_name.find('lm_head') != -1:\n",
    "            pass\n",
    "            #print(layer_name)\n",
    "        else:\n",
    "            # print(layer_name)\n",
    "            for layer_id in layer_ids:\n",
    "                if str(layer_id) in layer_name.split('.'):\n",
    "            # heads_range = range(n_heads)\n",
    "                    input[:, :, 1:, token_id] = 0\n",
    "            # sum_input = torch.unsqueeze(torch.sum(input, dim=-1), dim=-1)\n",
    "            # sum_input[:,:,0,:] = 1\n",
    "            # input = input / sum_input\n",
    "        return input\n",
    "\n",
    "    return modify_output, modify_input\n",
    "\n",
    "device = 'cuda:1'\n",
    "model_name = 'EleutherAI/gpt-j-6b'  # # 'EleutherAI/gpt-j-6b' 'meta-llama/Llama-2-7b'\n",
    "task = None\n",
    "model, tokenizer, model_config = load_gpt_model_and_tokenizer(model_name, device, True)\n",
    "# result = run_eval_harness(model, tokenizer, \"normal_gpt_j\", [\"winogrande\"], torch.device(device), 4)\n",
    "sink_token = None\n",
    "layer_ids = range(4,model_config['n_layers']-2)\n",
    "print(layer_ids)\n",
    "modify_output, modify_input = modify_1(token_id=0, layer_ids=layer_ids)\n",
    "with TraceDict2(model, layers=model_config['layer_hook_names'], edit_input=modify_input,\n",
    "                edit_output=modify_output, retain_output=False) as ret:\n",
    "    result2 = run_eval_harness(model, tokenizer, \"test_gpt_j\",\n",
    "                               task,torch.device(device), 4, sink_token=sink_token)\n",
    "result3 = run_eval_harness(model, tokenizer, \"normal_gpt_j\",\n",
    "                           task,torch.device(device), 4, sink_token=sink_token)\n",
    "print(result3)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be97004",
   "metadata": {},
   "source": [
    "### gpt-j with sink token '\\n': normal vs 0-27 zero vs 4-26 zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d398e44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:  EleutherAI/gpt-j-6b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 27)\n",
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 71026/71026 [52:29<00:00, 22.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(4, 26)\n",
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 71026/71026 [52:32<00:00, 22.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 71026/71026 [52:42<00:00, 22.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'results': {'anli_r1': {'acc': 0.349, 'acc_stderr': 0.015080663991563102}, 'anli_r2': {'acc': 0.348, 'acc_stderr': 0.015070604603768408}, 'anli_r3': {'acc': 0.365, 'acc_stderr': 0.013903485981413582}, 'hellaswag': {'acc': 0.48157737502489545, 'acc_stderr': 0.0049863932662691625, 'acc_norm': 0.6285600477992431, 'acc_norm_stderr': 0.004822022254886017}, 'piqa': {'acc': 0.750272034820457, 'acc_stderr': 0.010099232969867488, 'acc_norm': 0.7486398258977149, 'acc_norm_stderr': 0.010121156016819245}, 'winogrande': {'acc': 0.6448303078137332, 'acc_stderr': 0.013450047479569254}, 'wsc': {'acc': 0.36538461538461536, 'acc_stderr': 0.0474473339327792}, 'mathqa': {'acc': 0.25192629815745393, 'acc_stderr': 0.007947115720531424, 'acc_norm': 0.25862646566164155, 'acc_norm_stderr': 0.008015961308376573}}, 'versions': {'anli_r1': 0, 'anli_r2': 0, 'anli_r3': 0, 'hellaswag': 0, 'piqa': 0, 'winogrande': 0, 'wsc': 0, 'mathqa': 0}, 'config': {'name': 'normal_gpt_j'}}\n",
      "{'results': {'anli_r1': {'acc': 0.344, 'acc_stderr': 0.015029633724408945}, 'anli_r2': {'acc': 0.329, 'acc_stderr': 0.014865395385928355}, 'anli_r3': {'acc': 0.3383333333333333, 'acc_stderr': 0.013664144006618275}, 'hellaswag': {'acc': 0.43009360685122483, 'acc_stderr': 0.004940771559475497, 'acc_norm': 0.5537741485759808, 'acc_norm_stderr': 0.0049608399860995335}, 'piqa': {'acc': 0.7285092491838956, 'acc_stderr': 0.010376251176596135, 'acc_norm': 0.719804134929271, 'acc_norm_stderr': 0.010478122015577093}, 'winogrande': {'acc': 0.6266771902131019, 'acc_stderr': 0.013594002763035521}, 'wsc': {'acc': 0.36538461538461536, 'acc_stderr': 0.0474473339327792}, 'mathqa': {'acc': 0.24455611390284757, 'acc_stderr': 0.00786848204783649, 'acc_norm': 0.24991624790619765, 'acc_norm_stderr': 0.007925975319478038}}, 'versions': {'anli_r1': 0, 'anli_r2': 0, 'anli_r3': 0, 'hellaswag': 0, 'piqa': 0, 'winogrande': 0, 'wsc': 0, 'mathqa': 0}, 'config': {'name': 'test_gpt_j_0_27zero'}}\n",
      "{'results': {'anli_r1': {'acc': 0.342, 'acc_stderr': 0.01500870618212173}, 'anli_r2': {'acc': 0.336, 'acc_stderr': 0.014944140233795027}, 'anli_r3': {'acc': 0.3358333333333333, 'acc_stderr': 0.01363926119093288}, 'hellaswag': {'acc': 0.43158733320055764, 'acc_stderr': 0.004942853459371553, 'acc_norm': 0.5585540728938458, 'acc_norm_stderr': 0.004955447564694058}, 'piqa': {'acc': 0.7323177366702938, 'acc_stderr': 0.01033011118937043, 'acc_norm': 0.721436343852013, 'acc_norm_stderr': 0.01045939723596516}, 'winogrande': {'acc': 0.6424625098658248, 'acc_stderr': 0.01347000744392069}, 'wsc': {'acc': 0.36538461538461536, 'acc_stderr': 0.0474473339327792}, 'mathqa': {'acc': 0.24522613065326634, 'acc_stderr': 0.007875758516984984, 'acc_norm': 0.25125628140703515, 'acc_norm_stderr': 0.007940094121504884}}, 'versions': {'anli_r1': 0, 'anli_r2': 0, 'anli_r3': 0, 'hellaswag': 0, 'piqa': 0, 'winogrande': 0, 'wsc': 0, 'mathqa': 0}, 'config': {'name': 'test_gpt_j_4_26zero'}}\n"
     ]
    }
   ],
   "source": [
    "def modify_1(token_id, layer_ids):\n",
    "    def modify_output(output, layer_name, inputs):\n",
    "        # current_layer = int(layer_name.split(\".\")[2])\n",
    "        # if current_layer == edit_layer:\n",
    "        #     if isinstance(output, tuple):\n",
    "        #         output[0][:, idx] += fv_vector.to(device)\n",
    "        #         return output\n",
    "        #     else:\n",
    "        #         return output\n",
    "        # else:\n",
    "        #     return output\n",
    "        return output\n",
    "\n",
    "    def modify_input(input, layer_name:str):\n",
    "        if layer_name.find('wte') != -1:\n",
    "            pass\n",
    "            #print(layer_name)\n",
    "        elif layer_name.find('lm_head') != -1:\n",
    "            pass\n",
    "            #print(layer_name)\n",
    "        else:\n",
    "            # print(layer_name)\n",
    "            for layer_id in layer_ids:\n",
    "                if str(layer_id) in layer_name.split('.'):\n",
    "            # heads_range = range(n_heads)\n",
    "                    input[:, :, 1:, token_id] = 0\n",
    "            # sum_input = torch.unsqueeze(torch.sum(input, dim=-1), dim=-1)\n",
    "            # sum_input[:,:,0,:] = 1\n",
    "            # input = input / sum_input\n",
    "        return input\n",
    "\n",
    "    return modify_output, modify_input\n",
    "\n",
    "device = 'cuda:1'\n",
    "model_name = 'EleutherAI/gpt-j-6b'  # # 'EleutherAI/gpt-j-6b' 'meta-llama/Llama-2-7b'\n",
    "task = None\n",
    "model, tokenizer, model_config = load_gpt_model_and_tokenizer(model_name, device, True)\n",
    "# result = run_eval_harness(model, tokenizer, \"normal_gpt_j\", [\"winogrande\"], torch.device(device), 4)\n",
    "sink_token = '\\n'\n",
    "# 0-27zero\n",
    "layer_ids = range(model_config['n_layers']-1)\n",
    "print(layer_ids)\n",
    "modify_output, modify_input = modify_1(token_id=0, layer_ids=layer_ids)\n",
    "with TraceDict2(model, layers=model_config['layer_hook_names'], edit_input=modify_input,\n",
    "                edit_output=modify_output, retain_output=False) as ret:\n",
    "    result1 = run_eval_harness(model, tokenizer, \"test_gpt_j_0_27zero\",\n",
    "                               task,torch.device(device), 4, sink_token=sink_token)\n",
    "# 4-26zero\n",
    "layer_ids = range(4,model_config['n_layers']-2)\n",
    "print(layer_ids)\n",
    "modify_output, modify_input = modify_1(token_id=0, layer_ids=layer_ids)\n",
    "with TraceDict2(model, layers=model_config['layer_hook_names'], edit_input=modify_input,\n",
    "                edit_output=modify_output, retain_output=False) as ret:\n",
    "    result2 = run_eval_harness(model, tokenizer, \"test_gpt_j_4_26zero\",\n",
    "                               task,torch.device(device), 4, sink_token=sink_token)\n",
    "# normal\n",
    "result3 = run_eval_harness(model, tokenizer, \"normal_gpt_j\",\n",
    "                           task,torch.device(device), 4, sink_token=sink_token)\n",
    "print(result3)\n",
    "print(result1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c03d901a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:  EleutherAI/gpt-j-6b\n",
      "range(0, 27)\n",
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 71026/71026 [52:41<00:00, 22.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(4, 26)\n",
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 71026/71026 [52:32<00:00, 22.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'results': {'anli_r1': {'acc': 0.345, 'acc_stderr': 0.015039986742055235}, 'anli_r2': {'acc': 0.352, 'acc_stderr': 0.015110404505648668}, 'anli_r3': {'acc': 0.35583333333333333, 'acc_stderr': 0.01382651874849332}, 'hellaswag': {'acc': 0.4816769567815176, 'acc_stderr': 0.004986429808146773, 'acc_norm': 0.6255725951005776, 'acc_norm_stderr': 0.004829856058603566}, 'piqa': {'acc': 0.7513601741022851, 'acc_stderr': 0.010084511234296855, 'acc_norm': 0.7453754080522307, 'acc_norm_stderr': 0.010164432237060506}, 'winogrande': {'acc': 0.6495659037095501, 'acc_stderr': 0.013409047676670182}, 'wsc': {'acc': 0.36538461538461536, 'acc_stderr': 0.0474473339327792}, 'mathqa': {'acc': 0.24690117252931323, 'acc_stderr': 0.007893836965752426, 'acc_norm': 0.25728643216080405, 'acc_norm_stderr': 0.00800238997449364}}, 'versions': {'anli_r1': 0, 'anli_r2': 0, 'anli_r3': 0, 'hellaswag': 0, 'piqa': 0, 'winogrande': 0, 'wsc': 0, 'mathqa': 0}, 'config': {'name': 'test_gpt_j_0_27zero'}}\n",
      "{'results': {'anli_r1': {'acc': 0.331, 'acc_stderr': 0.01488827258820393}, 'anli_r2': {'acc': 0.322, 'acc_stderr': 0.014782913600996673}, 'anli_r3': {'acc': 0.3308333333333333, 'acc_stderr': 0.013588208070709}, 'hellaswag': {'acc': 0.37283409679346746, 'acc_stderr': 0.004825702533920399, 'acc_norm': 0.46265684126667994, 'acc_norm_stderr': 0.004975845335086622}, 'piqa': {'acc': 0.6599564744287268, 'acc_stderr': 0.01105274941442355, 'acc_norm': 0.6360174102285092, 'acc_norm_stderr': 0.011225875703487171}, 'winogrande': {'acc': 0.5367008681925809, 'acc_stderr': 0.01401457845884326}, 'wsc': {'acc': 0.41346153846153844, 'acc_stderr': 0.04852294969729053}, 'mathqa': {'acc': 0.20871021775544388, 'acc_stderr': 0.007439439650874906, 'acc_norm': 0.2234505862646566, 'acc_norm_stderr': 0.007625632786177484}}, 'versions': {'anli_r1': 0, 'anli_r2': 0, 'anli_r3': 0, 'hellaswag': 0, 'piqa': 0, 'winogrande': 0, 'wsc': 0, 'mathqa': 0}, 'config': {'name': 'test_gpt_j_4_26zero'}}\n"
     ]
    }
   ],
   "source": [
    "def modify_1(token_id, layer_ids):\n",
    "    def modify_output(output, layer_name, inputs):\n",
    "        # current_layer = int(layer_name.split(\".\")[2])\n",
    "        # if current_layer == edit_layer:\n",
    "        #     if isinstance(output, tuple):\n",
    "        #         output[0][:, idx] += fv_vector.to(device)\n",
    "        #         return output\n",
    "        #     else:\n",
    "        #         return output\n",
    "        # else:\n",
    "        #     return output\n",
    "        return output\n",
    "\n",
    "    def modify_input(input, layer_name:str):\n",
    "        if layer_name.find('wte') != -1:\n",
    "            pass\n",
    "            #print(layer_name)\n",
    "        elif layer_name.find('lm_head') != -1:\n",
    "            pass\n",
    "            #print(layer_name)\n",
    "        else:\n",
    "            # print(layer_name)\n",
    "            for layer_id in layer_ids:\n",
    "                if str(layer_id) in layer_name.split('.'):\n",
    "            # heads_range = range(n_heads)\n",
    "                    input[:, :, 1:, token_id] = 0\n",
    "                    sum_input = torch.unsqueeze(torch.sum(input, dim=-1), dim=-1)\n",
    "                    sum_input[:,:,0,:] = 1\n",
    "                    input = input / sum_input\n",
    "        return input\n",
    "\n",
    "    return modify_output, modify_input\n",
    "\n",
    "device = 'cuda:1'\n",
    "model_name = 'EleutherAI/gpt-j-6b'  # # 'EleutherAI/gpt-j-6b' 'meta-llama/Llama-2-7b'\n",
    "task = None\n",
    "model, tokenizer, model_config = load_gpt_model_and_tokenizer(model_name, device, True)\n",
    "# result = run_eval_harness(model, tokenizer, \"normal_gpt_j\", [\"winogrande\"], torch.device(device), 4)\n",
    "sink_token = '\\n'\n",
    "# 0-27zero\n",
    "layer_ids = range(model_config['n_layers']-1)\n",
    "print(layer_ids)\n",
    "modify_output, modify_input = modify_1(token_id=0, layer_ids=layer_ids)\n",
    "with TraceDict2(model, layers=model_config['layer_hook_names'], edit_input=modify_input,\n",
    "                edit_output=modify_output, retain_output=False) as ret:\n",
    "    result1 = run_eval_harness(model, tokenizer, \"test_gpt_j_0_27zero\",\n",
    "                               task,torch.device(device), 4, sink_token=sink_token)\n",
    "# 4-26zero\n",
    "layer_ids = range(4,model_config['n_layers']-2)\n",
    "print(layer_ids)\n",
    "modify_output, modify_input = modify_1(token_id=0, layer_ids=layer_ids)\n",
    "with TraceDict2(model, layers=model_config['layer_hook_names'], edit_input=modify_input,\n",
    "                edit_output=modify_output, retain_output=False) as ret:\n",
    "    result2 = run_eval_harness(model, tokenizer, \"test_gpt_j_4_26zero\",\n",
    "                               task,torch.device(device), 4, sink_token=sink_token)\n",
    "\n",
    "print(result1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238bce3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e92f07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:  EleutherAI/gpt-j-6b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 27)\n",
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 71026/71026 [52:33<00:00, 22.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(4, 26)\n",
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████████████████████                                                                                         | 21644/71026 [22:51<39:52, 20.64it/s]"
     ]
    }
   ],
   "source": [
    "def modify_1(token_id, layer_ids):\n",
    "    def modify_output(output, layer_name, inputs):\n",
    "        # current_layer = int(layer_name.split(\".\")[2])\n",
    "        # if current_layer == edit_layer:\n",
    "        #     if isinstance(output, tuple):\n",
    "        #         output[0][:, idx] += fv_vector.to(device)\n",
    "        #         return output\n",
    "        #     else:\n",
    "        #         return output\n",
    "        # else:\n",
    "        #     return output\n",
    "        return output\n",
    "\n",
    "    def modify_input(input, layer_name:str):\n",
    "        if layer_name.find('wte') != -1:\n",
    "            pass\n",
    "            #print(layer_name)\n",
    "        elif layer_name.find('lm_head') != -1:\n",
    "            pass\n",
    "            #print(layer_name)\n",
    "        else:\n",
    "            # print(layer_name)\n",
    "            for layer_id in layer_ids:\n",
    "                if str(layer_id) in layer_name.split('.'):\n",
    "            # heads_range = range(n_heads)\n",
    "                    input[:, :, 1:, token_id] = 0\n",
    "                    sum_input = torch.unsqueeze(torch.sum(input, dim=-1), dim=-1)*2\n",
    "                    sum_input[:,:,0,:] = 1\n",
    "                    input = input / sum_input\n",
    "        return input\n",
    "\n",
    "    return modify_output, modify_input\n",
    "\n",
    "device = 'cuda:1'\n",
    "model_name = 'EleutherAI/gpt-j-6b'  # # 'EleutherAI/gpt-j-6b' 'meta-llama/Llama-2-7b'\n",
    "task = None\n",
    "model, tokenizer, model_config = load_gpt_model_and_tokenizer(model_name, device, True)\n",
    "# result = run_eval_harness(model, tokenizer, \"normal_gpt_j\", [\"winogrande\"], torch.device(device), 4)\n",
    "sink_token = '\\n'\n",
    "# 0-27zero\n",
    "layer_ids = range(model_config['n_layers']-1)\n",
    "print(layer_ids)\n",
    "modify_output, modify_input = modify_1(token_id=0, layer_ids=layer_ids)\n",
    "with TraceDict2(model, layers=model_config['layer_hook_names'], edit_input=modify_input,\n",
    "                edit_output=modify_output, retain_output=False) as ret:\n",
    "    result1 = run_eval_harness(model, tokenizer, \"test_gpt_j_0_27zero\",\n",
    "                               task,torch.device(device), 4, sink_token=sink_token)\n",
    "# 4-26zero\n",
    "layer_ids = range(4,model_config['n_layers']-2)\n",
    "print(layer_ids)\n",
    "modify_output, modify_input = modify_1(token_id=0, layer_ids=layer_ids)\n",
    "with TraceDict2(model, layers=model_config['layer_hook_names'], edit_input=modify_input,\n",
    "                edit_output=modify_output, retain_output=False) as ret:\n",
    "    result2 = run_eval_harness(model, tokenizer, \"test_gpt_j_4_26zero\",\n",
    "                               task,torch.device(device), 4, sink_token=sink_token)\n",
    "\n",
    "print(result1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13427d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_1(token_id, layer_ids):\n",
    "    def modify_output(output, layer_name, inputs):\n",
    "        # current_layer = int(layer_name.split(\".\")[2])\n",
    "        # if current_layer == edit_layer:\n",
    "        #     if isinstance(output, tuple):\n",
    "        #         output[0][:, idx] += fv_vector.to(device)\n",
    "        #         return output\n",
    "        #     else:\n",
    "        #         return output\n",
    "        # else:\n",
    "        #     return output\n",
    "        return output\n",
    "\n",
    "    def modify_input(input, layer_name:str):\n",
    "        if layer_name.find('wte') != -1:\n",
    "            pass\n",
    "            #print(layer_name)\n",
    "        elif layer_name.find('lm_head') != -1:\n",
    "            pass\n",
    "            #print(layer_name)\n",
    "        else:\n",
    "            # print(layer_name)\n",
    "            for layer_id in layer_ids:\n",
    "                if str(layer_id) in layer_name.split('.'):\n",
    "            # heads_range = range(n_heads)\n",
    "                    input[:, :, 1:, token_id] = 0\n",
    "                    sum_input = torch.unsqueeze(torch.sum(input, dim=-1), dim=-1)/2\n",
    "                    sum_input[:,:,0,:] = 1\n",
    "                    input = input / sum_input\n",
    "        return input\n",
    "\n",
    "    return modify_output, modify_input\n",
    "\n",
    "device = 'cuda:1'\n",
    "model_name = 'EleutherAI/gpt-j-6b'  # # 'EleutherAI/gpt-j-6b' 'meta-llama/Llama-2-7b'\n",
    "task = None\n",
    "model, tokenizer, model_config = load_gpt_model_and_tokenizer(model_name, device, True)\n",
    "# result = run_eval_harness(model, tokenizer, \"normal_gpt_j\", [\"winogrande\"], torch.device(device), 4)\n",
    "sink_token = '\\n'\n",
    "# 0-27zero\n",
    "layer_ids = range(model_config['n_layers']-1)\n",
    "print(layer_ids)\n",
    "modify_output, modify_input = modify_1(token_id=0, layer_ids=layer_ids)\n",
    "with TraceDict2(model, layers=model_config['layer_hook_names'], edit_input=modify_input,\n",
    "                edit_output=modify_output, retain_output=False) as ret:\n",
    "    result1 = run_eval_harness(model, tokenizer, \"test_gpt_j_0_27zero\",\n",
    "                               task,torch.device(device), 4, sink_token=sink_token)\n",
    "# 4-26zero\n",
    "layer_ids = range(4,model_config['n_layers']-2)\n",
    "print(layer_ids)\n",
    "modify_output, modify_input = modify_1(token_id=0, layer_ids=layer_ids)\n",
    "with TraceDict2(model, layers=model_config['layer_hook_names'], edit_input=modify_input,\n",
    "                edit_output=modify_output, retain_output=False) as ret:\n",
    "    result2 = run_eval_harness(model, tokenizer, \"test_gpt_j_4_26zero\",\n",
    "                               task,torch.device(device), 4, sink_token=sink_token)\n",
    "\n",
    "print(result1)\n",
    "print(result2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
