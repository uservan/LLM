{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 13:23:55.044163: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-15 13:23:55.078065: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-15 13:23:55.680509: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM,\n",
    "                          GPTNeoForCausalLM, GPT2TokenizerFast,LlamaConfig)\n",
    "import os\n",
    "import random\n",
    "from typing import *\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer, BloomForCausalLM\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- audio-classification\n",
      "---------- automatic-speech-recognition\n",
      "---------- text-to-audio\n",
      "---------- feature-extraction\n",
      "---------- text-classification\n",
      "---------- token-classification\n",
      "---------- question-answering\n",
      "---------- table-question-answering\n",
      "---------- visual-question-answering\n",
      "---------- document-question-answering\n",
      "---------- fill-mask\n",
      "---------- summarization\n",
      "---------- translation\n",
      "---------- text2text-generation\n",
      "---------- text-generation\n",
      "---------- zero-shot-classification\n",
      "---------- zero-shot-image-classification\n",
      "---------- zero-shot-audio-classification\n",
      "---------- conversational\n",
      "---------- image-classification\n",
      "---------- image-segmentation\n",
      "---------- image-to-text\n",
      "---------- object-detection\n",
      "---------- zero-shot-object-detection\n",
      "---------- depth-estimation\n",
      "---------- video-classification\n",
      "---------- mask-generation\n",
      "---------- image-to-image\n"
     ]
    }
   ],
   "source": [
    "from transformers.pipelines import SUPPORTED_TASKS\n",
    "for k, v in SUPPORTED_TASKS.items():\n",
    "    print('----------',k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGPTNeoSelfAttention(nn.Module):\n",
    "    def __init__(self, config, device, is_linear):\n",
    "        super().__init__()\n",
    "\n",
    "        max_positions = config.max_position_embeddings\n",
    "        # bias = torch.tril(torch.ones((max_positions, max_positions), dtype=bool)).view(\n",
    "        #     1, 1, max_positions, max_positions\n",
    "        # )\n",
    "\n",
    "        # local causal self attention is a sliding window where each token can only attend to the previous\n",
    "        # window_size tokens. This is implemented by updating the causal mask such that for each token\n",
    "        # all other tokens are masked except the previous window_size tokens.\n",
    "        # if attention_type == \"local\":\n",
    "        #     bias = torch.bitwise_xor(bias, torch.tril(bias, -config.window_size))\n",
    "\n",
    "        # self.attn_dropout = nn.Dropout(float(config.attention_dropout))\n",
    "        self.resid_dropout = nn.Dropout(float(config.resid_dropout))\n",
    "\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        if self.head_dim * self.num_heads != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n",
    "                f\" {self.num_heads}).\"\n",
    "            )\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(in_features=self.embed_dim, out_features=self.embed_dim*2, bias=False, dtype=torch.float16),\n",
    "                                    nn.Linear(in_features=self.embed_dim*2, out_features=self.embed_dim, bias=False, dtype=torch.float16))\n",
    "        from torch.nn.init import xavier_uniform_\n",
    "        xavier_uniform_(self.linear[0].weight.data)\n",
    "        xavier_uniform_(self.linear[1].weight.data)\n",
    "        self.device = device\n",
    "        self.is_linear = is_linear\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        layer_past=None,\n",
    "        head_mask=None,\n",
    "        use_cache=False,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "        input_dtype = hidden_states.dtype\n",
    "        if input_dtype != torch.float16:\n",
    "            self.linear = self.linear.to(input_dtype)\n",
    "\n",
    "        key = torch.ones((bsz, q_len, self.num_heads, self.head_dim), dtype=input_dtype).to(\n",
    "            self.device).transpose(1, 2)\n",
    "        value = torch.ones((bsz, q_len, self.num_heads, self.head_dim), dtype=input_dtype).to(\n",
    "            self.device).transpose(1, 2)\n",
    "\n",
    "        attn_weights = torch.zeros((bsz, self.num_heads, q_len, q_len), dtype=input_dtype).to(self.device)\n",
    "\n",
    "        attn_output = self.linear(hidden_states)\n",
    "        p = self.linear.parameters()\n",
    "        if self.is_linear:\n",
    "            # attn_output = self.resid_dropout(attn_output)\n",
    "            pass\n",
    "        else:\n",
    "            attn_output = torch.ones_like(attn_output, dtype=input_dtype).to(self.device)\n",
    "\n",
    "        if layer_past is not None:\n",
    "            past_key = layer_past[0]\n",
    "            past_value = layer_past[1]\n",
    "            key = torch.cat((past_key, key), dim=-2)\n",
    "            value = torch.cat((past_value, value), dim=-2)\n",
    "\n",
    "        if use_cache is True:\n",
    "            present = (key, value)\n",
    "        else:\n",
    "            present = None\n",
    "\n",
    "        outputs = (attn_output, present)\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights,)\n",
    "\n",
    "        return outputs  # a, present, (attentions)\n",
    "\n",
    "class MyLlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config: LlamaConfig, device, is_linear):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.rope_theta = config.rope_theta\n",
    "        self.is_causal = True\n",
    "\n",
    "        # self.resid_dropout = nn.Dropout(float(config.resid_dropout))\n",
    "        self.linear = nn.Sequential(nn.Linear(in_features=2048, out_features=5632, bias=False, dtype=torch.float16),\n",
    "                                                      nn.Linear(in_features=5632, out_features=2048, bias=False, dtype=torch.float16))\n",
    "        self.device = device\n",
    "        self.is_linear =is_linear\n",
    "        if (self.head_dim * self.num_heads) != self.hidden_size:\n",
    "            raise ValueError(\n",
    "                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n",
    "                f\" and `num_heads`: {self.num_heads}).\"\n",
    "            )\n",
    "\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "        input_dtype = hidden_states.dtype\n",
    "        if input_dtype != torch.float16:\n",
    "            self.linear = self.linear.to(input_dtype)\n",
    "\n",
    "\n",
    "        key_states = torch.zeros((bsz, q_len, self.num_key_value_heads, self.head_dim), dtype=input_dtype).to(self.device).transpose(1, 2)\n",
    "        value_states = torch.zeros((bsz, q_len, self.num_key_value_heads, self.head_dim), dtype=input_dtype).to(self.device).transpose(1, 2)\n",
    "\n",
    "        past_key_value = (key_states, value_states) if use_cache else None\n",
    "\n",
    "        attn_weights = torch.zeros((bsz, self.num_heads, q_len, q_len), dtype=input_dtype).to(self.device)\n",
    "\n",
    "        attn_output = self.linear(hidden_states)\n",
    "        if not self.is_linear:\n",
    "            attn_output = torch.zeros_like(attn_output, dtype=input_dtype).to(self.device)\n",
    "\n",
    "        return attn_output, attn_weights, past_key_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_heatMap_sns(scores, save_path=None, title=None, cmap=None, y_ticks=None, x_ticks=None, show=None):\n",
    "    plt.subplots(figsize=(20, 20), dpi=200)\n",
    "    plt.rcParams['font.size'] = '10'\n",
    "    if cmap is None:\n",
    "        cmap = sns.color_palette(\"Reds\", as_cmap=True)\n",
    "    if x_ticks and y_ticks:\n",
    "        sns.heatmap(scores, cmap=cmap,  xticklabels=x_ticks, yticklabels=y_ticks)\n",
    "    else:\n",
    "        sns.heatmap(scores, cmap=cmap)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(os.path.join(save_path, f'{title}.png'), bbox_inches=\"tight\")\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def load_model(model_name: str, device='cuda', low_cpu_mem_usage=False, layers=[], train_layers=[], is_linear=False):\n",
    "    tokenizer: GPT2TokenizerFast = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    if model_name.find('gpt-neo') != -1:\n",
    "        model: GPTNeoForCausalLM = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)# .to(device)\n",
    "\n",
    "        for layer_id in layers:\n",
    "            model.transformer.h[layer_id].attn.attention = MyGPTNeoSelfAttention(model.config, device=device,\n",
    "                                                                                 is_linear=is_linear).to(device)\n",
    "            # model.model.layers[layer_id].self_attn = LlamaAttentionLinear(model.config , device).to(device)\n",
    "\n",
    "        MODEL_CONFIG = {\"n_heads\": model.config.num_attention_heads,\n",
    "                        \"n_layers\": model.config.num_hidden_layers,\n",
    "                        \"resid_dim\": model.config.hidden_size,\n",
    "                        \"name_or_path\": model.config.name_or_path,\n",
    "                        \"attn_hook_names\": [f'transformer.h.{layer}.attn.attention.attn_dropout' for layer in\n",
    "                                            range(model.config.num_hidden_layers)]\n",
    "                        }\n",
    "        # p = model.named_parameters()\n",
    "        if len(train_layers) > 0:\n",
    "            for k, params in model.named_parameters():\n",
    "                flag = False\n",
    "                for layer_id in train_layers:\n",
    "                    if is_linear:\n",
    "                        name = f'transformer.h.{layer_id}.attn.attention'\n",
    "                    else:\n",
    "                        name = f'transformer.h.{layer_id}.mlp'\n",
    "                    if k.find(name) != -1:\n",
    "                        flag = True\n",
    "                params.requires_grad = flag\n",
    "\n",
    "        return model, tokenizer, MODEL_CONFIG\n",
    "    if model_name.find('TinyLlama') != -1:\n",
    "        model: LlamaForCausalLM = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)#.to(device)\n",
    "\n",
    "        for layer_id in layers:\n",
    "            # model.transformer.h[layer_id].attn.attention = MyGPTNeoSelfAttention(model.config, device=device,is_linear=is_linear).to(device)\n",
    "            model.model.layers[layer_id].self_attn = MyLlamaAttention(model.config, device, is_linear=is_linear).to(\n",
    "                device)\n",
    "\n",
    "        MODEL_CONFIG = {\"n_heads\": model.config.num_attention_heads,\n",
    "                        \"n_layers\": model.config.num_hidden_layers,\n",
    "                        \"resid_dim\": model.config.hidden_size,\n",
    "                        \"name_or_path\": model.config.name_or_path,\n",
    "                        \"attn_hook_names\": [f'model.layers.{layer}.self_attn.attn_dropout' for layer in\n",
    "                                            range(model.config.num_hidden_layers)]\n",
    "                        }\n",
    "        if len(train_layers) > 0:\n",
    "            for k, params in model.named_parameters():\n",
    "                for layer_id in train_layers:\n",
    "                    if k.find(str(layer_id)) == -1:\n",
    "                        params.requires_grad = False\n",
    "\n",
    "        return model, tokenizer, MODEL_CONFIG\n",
    "\n",
    "\n",
    "def draw_attention(model, tokenizer, MODEL_CONFIG, device, prompt, check_token_id, save_path, title):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    output_and_cache = model(**inputs, output_hidden_states=True, output_attentions=True)\n",
    "    ground_attentions = torch.cat(output_and_cache.attentions, dim=0).detach().cpu().numpy()\n",
    "    ground_attentions = ground_attentions[:, :, check_token_id, :]\n",
    "\n",
    "    x_ticks = [f\"layer{i + 1}\" for i in range(MODEL_CONFIG['n_layers'])]\n",
    "    encoded_line = tokenizer.encode(prompt)\n",
    "    codes = tokenizer.convert_ids_to_tokens(encoded_line)\n",
    "    y_ticks = [f\"head{i_head}-{c}\" for i_head in range(MODEL_CONFIG['n_heads']) for i, c in enumerate(codes)]\n",
    "    plt_heatMap_sns(ground_attentions.reshape(ground_attentions.shape[0], -1).T,\n",
    "                    title=title, x_ticks=x_ticks, y_ticks=y_ticks\n",
    "                    , show=True, save_path=save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [22]  # [23]\n",
    "train_layers = layers\n",
    "is_linear = False\n",
    "save_model_path = f'./results/gpt_neo_{layers}_{is_linear}_{train_layers}'\n",
    "\n",
    "device_str = 'cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model_name = 'EleutherAI/gpt-neo-1.3B'  # 'EleutherAI/gpt-neo-1.3B' # 'TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T' # 'EleutherAI/gpt-neo-125m'\n",
    "model, tokenizer, MODEL_CONFIG = load_model(model_name, device=device_str, layers=layers, train_layers=train_layers,\n",
    "                                            is_linear=is_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='EleutherAI/gpt-neo-1.3B', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)\n",
    "# MODEL_CONFIG\n",
    "print(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50257, 2048)\n",
       "    (wpe): Embedding(2048, 2048)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-21): 22 x GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): MyGPTNeoSelfAttention(\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (linear): Sequential(\n",
       "              (0): Linear(in_features=2048, out_features=4096, bias=False)\n",
       "              (1): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As far as I am concerned, I will be back in the future. I just had to wait until the other team members can give me their opinion before I decided for myself.\n",
      "\n",
      "I have to say that I'm really looking forward to it especially since this series will be my first-ever anime. It will definitely be a fresh start on both our side :) I think I'll keep this blog for a long time :)\n",
      "\n",
      "For my birthday, I bought a new laptop for my computer lab...\n",
      "and it was so convenient as it has dual monitor support. We are currently using a Macbook Pro and I was wondering if\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# print(sys.path)\n",
    "import os\n",
    "sys.path.append(os.path.join(sys.path[0], '../'))\n",
    "from utils.evaluation_lm_eval import run_eval_harness\n",
    "from gpt_neo import load_model\n",
    "\n",
    "\n",
    "# model.load_state_dict(torch.load(save_model_path+'pth'))\n",
    "model = model.to(torch.device(device_str))\n",
    "# result2 = run_eval_harness(model, tokenizer, \"test_gpt_j\",None, torch.device(device), 4, sink_token=None)\n",
    "\n",
    "from transformers import pipeline\n",
    "#文本生成\n",
    "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=device_str, pad_token_id=tokenizer.eos_token_id)\n",
    "results= text_generator(\"As far as I am concerned, I will\",\n",
    "\t\t\t   max_length=128,\n",
    "\t\t\t   do_sample=True)\n",
    "print(results[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e49c32a6d2485992201da89e7401a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " # dataset = load_dataset('cerebras/SlimPajama-627B',split='train[:100]') # TinyLlama\n",
    "dataset = load_dataset('monology/pile-uncopyrighted', split='train[:1%]')  # gpt-neo\n",
    "\n",
    "def process_func(examples):\n",
    "    contents = [e + tokenizer.eos_token for e in examples[\"text\"]]\n",
    "    return tokenizer(contents, max_length=128, truncation=True)\n",
    "tokenized_ds = dataset.map(process_func, batched=True, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1770097"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   32,  4950, 38504,   279, 23048,  1657,    11, 10691,   422,   262,\n",
      "        16236,   447,   247,    82,    13,   383,  1115,    12,  2971,   279,\n",
      "        23048,  3033,   257, 21682,   276,    11, 37204, 29500, 17979, 41860,\n",
      "          351,   257,  7932,   300,   813, 15061,  1486,    13,   383, 17979,\n",
      "          468,   257,  1598,  3641,   351,   281, 12531,  1486,    11,   351,\n",
      "          257,   307,  5286,  4865,  1088,   262,  5743,   286,   262, 17979,\n",
      "           13, 23302,   262, 17979,   318,   257,  1598,    11,  3652,  5819,\n",
      "           12, 16760,  5405,  2665,   351,   257,  5405,  9396,   284,   543,\n",
      "          262,  6333,   318,  7223,    13,   770,  1657, 29220,   318,   287,\n",
      "         6275,  4006,    26,   340,   468,   587, 28049,   302, 44236,   290,\n",
      "         2058,   351,   477,   262,  3306, 32161,   329,  3660,  9988,    13,\n",
      "          383,  5405,   318,   635,   287,  6275,  4006,    11,   351,   645,\n",
      "        23217,   393,  9457,    11,   655,   262,  2938,  5104])\n",
      "A beautiful antique pendant light, dating from the 1940’s. The three-light pendant features a frosted, inverted dome shade adorned with a wonderful lily flower design. The shade has a clear center with an abstract design, with a beaded border around the edge of the shade. Above the shade is a clear, baluster-shaped glass section with a glass bowl to which the chain is attached. This light fixture is in excellent condition; it has been professionally rewired and comes with all the necessary attachments for modern installation. The glass is also in excellent condition, with no cracks or breaks, just the expected fle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [32, 4950, 38504, 279, 23048, 1657, 11, 10691, 422, 262, 16236, 447, 247, 82, 13, 383, 1115, 12, 2971, 279, 23048, 3033, 257, 21682, 276, 11, 37204, 29500, 17979, 41860, 351, 257, 7932, 300, 813, 15061, 1486, 13, 383, 17979, 468, 257, 1598, 3641, 351, 281, 12531, 1486, 11, 351, 257, 307, 5286, 4865, 1088, 262, 5743, 286, 262, 17979, 13, 23302, 262, 17979, 318, 257, 1598, 11, 3652, 5819, 12, 16760, 5405, 2665, 351, 257, 5405, 9396, 284, 543, 262, 6333, 318, 7223, 13, 770, 1657, 29220, 318, 287, 6275, 4006, 26, 340, 468, 587, 28049, 302, 44236, 290, 2058, 351, 477, 262, 3306, 32161, 329, 3660, 9988, 13, 383, 5405, 318, 635, 287, 6275, 4006, 11, 351, 645, 23217, 393, 9457, 11, 655, 262, 2938, 5104], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(len(tokenized_ds['input_ids'][2]))\n",
    "# print(len(tokenized_ds['input_ids'][0]))\n",
    "dl = DataLoader(tokenized_ds, batch_size=16, collate_fn=DataCollatorForLanguageModeling(tokenizer, mlm=False), shuffle=True)\n",
    "inp = next(enumerate(dl))[1]['input_ids'][0]\n",
    "print(inp)\n",
    "print(tokenizer.decode(inp))\n",
    "tokenizer(tokenizer.decode(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, global_step: 0, loss: 2.298828125\n",
      "ep: 0, global_step: 100, loss: 2.626953125\n",
      "ep: 0, global_step: 200, loss: 2.2421875\n",
      "ep: 0, global_step: 300, loss: 2.150390625\n",
      "ep: 0, global_step: 400, loss: 2.244140625\n",
      "ep: 0, global_step: 500, loss: 2.103515625\n",
      "ep: 0, global_step: 600, loss: 2.73828125\n",
      "ep: 0, global_step: 700, loss: 2.197265625\n",
      "ep: 0, global_step: 800, loss: 2.59765625\n",
      "ep: 0, global_step: 900, loss: 2.103515625\n",
      "ep: 0, global_step: 1000, loss: 2.08984375\n",
      "ep: 0, global_step: 1100, loss: 2.21875\n",
      "ep: 0, global_step: 1200, loss: 2.27734375\n",
      "ep: 0, global_step: 1300, loss: 2.40234375\n",
      "ep: 0, global_step: 1400, loss: 2.267578125\n",
      "ep: 0, global_step: 1500, loss: 2.36328125\n",
      "ep: 0, global_step: 1600, loss: 2.416015625\n",
      "ep: 0, global_step: 1700, loss: 2.84375\n",
      "ep: 0, global_step: 1800, loss: 2.447265625\n",
      "ep: 0, global_step: 1900, loss: 2.583984375\n",
      "ep: 0, global_step: 2000, loss: 2.09375\n",
      "ep: 0, global_step: 2100, loss: 2.138671875\n",
      "ep: 0, global_step: 2200, loss: 2.556640625\n",
      "ep: 0, global_step: 2300, loss: 2.25\n",
      "ep: 0, global_step: 2400, loss: 2.080078125\n",
      "ep: 0, global_step: 2500, loss: 2.201171875\n",
      "ep: 0, global_step: 2600, loss: 2.125\n",
      "ep: 0, global_step: 2700, loss: 2.66015625\n",
      "ep: 0, global_step: 2800, loss: 2.326171875\n",
      "ep: 0, global_step: 2900, loss: 2.46875\n",
      "ep: 0, global_step: 3000, loss: 2.056640625\n",
      "ep: 0, global_step: 3100, loss: 2.439453125\n",
      "ep: 0, global_step: 3200, loss: 2.4375\n",
      "ep: 0, global_step: 3300, loss: 2.38671875\n",
      "ep: 0, global_step: 3400, loss: 2.818359375\n",
      "ep: 0, global_step: 3500, loss: 2.484375\n",
      "ep: 0, global_step: 3600, loss: 2.296875\n",
      "ep: 0, global_step: 3700, loss: 2.1640625\n",
      "ep: 0, global_step: 3800, loss: 1.9365234375\n",
      "ep: 0, global_step: 3900, loss: 2.26171875\n",
      "ep: 0, global_step: 4000, loss: 2.24609375\n",
      "ep: 0, global_step: 4100, loss: 2.08203125\n",
      "ep: 0, global_step: 4200, loss: 2.568359375\n",
      "ep: 0, global_step: 4300, loss: 2.19921875\n",
      "ep: 0, global_step: 4400, loss: 2.2578125\n",
      "ep: 0, global_step: 4500, loss: 1.9951171875\n",
      "ep: 0, global_step: 4600, loss: 2.63671875\n",
      "ep: 0, global_step: 4700, loss: 2.193359375\n",
      "ep: 0, global_step: 4800, loss: 2.072265625\n",
      "ep: 0, global_step: 4900, loss: 2.599609375\n",
      "ep: 0, global_step: 5000, loss: 2.333984375\n",
      "ep: 0, global_step: 5100, loss: 2.306640625\n",
      "ep: 0, global_step: 5200, loss: 2.30859375\n",
      "ep: 0, global_step: 5300, loss: 2.0703125\n",
      "ep: 0, global_step: 5400, loss: 2.00390625\n",
      "ep: 0, global_step: 5500, loss: 2.205078125\n",
      "ep: 0, global_step: 5600, loss: 1.955078125\n",
      "ep: 0, global_step: 5700, loss: 2.005859375\n",
      "ep: 0, global_step: 5800, loss: 2.11328125\n",
      "ep: 0, global_step: 5900, loss: 2.2265625\n",
      "ep: 0, global_step: 6000, loss: 2.396484375\n",
      "ep: 0, global_step: 6100, loss: 1.892578125\n",
      "ep: 0, global_step: 6200, loss: 2.169921875\n",
      "ep: 0, global_step: 6300, loss: 2.423828125\n",
      "ep: 0, global_step: 6400, loss: 2.75390625\n",
      "ep: 0, global_step: 6500, loss: 2.380859375\n",
      "ep: 0, global_step: 6600, loss: 2.326171875\n",
      "ep: 0, global_step: 6700, loss: 2.48828125\n",
      "ep: 0, global_step: 6800, loss: 2.37109375\n",
      "ep: 0, global_step: 6900, loss: 2.60546875\n",
      "ep: 0, global_step: 7000, loss: 2.076171875\n",
      "ep: 0, global_step: 7100, loss: 2.201171875\n",
      "ep: 0, global_step: 7200, loss: 2.1015625\n",
      "ep: 0, global_step: 7300, loss: 2.228515625\n",
      "ep: 0, global_step: 7400, loss: 2.15625\n",
      "ep: 0, global_step: 7500, loss: 2.51171875\n",
      "ep: 0, global_step: 7600, loss: 2.189453125\n",
      "ep: 0, global_step: 7700, loss: 2.658203125\n",
      "ep: 0, global_step: 7800, loss: 2.658203125\n",
      "ep: 0, global_step: 7900, loss: 2.48046875\n",
      "ep: 0, global_step: 8000, loss: 1.9521484375\n",
      "ep: 0, global_step: 8100, loss: 2.123046875\n",
      "ep: 0, global_step: 8200, loss: 2.439453125\n",
      "ep: 0, global_step: 8300, loss: 2.01953125\n",
      "ep: 0, global_step: 8400, loss: 2.234375\n",
      "ep: 0, global_step: 8500, loss: 2.41015625\n",
      "ep: 0, global_step: 8600, loss: 1.9482421875\n",
      "ep: 0, global_step: 8700, loss: 1.9267578125\n",
      "ep: 0, global_step: 8800, loss: 2.51171875\n",
      "ep: 0, global_step: 8900, loss: 2.396484375\n",
      "ep: 0, global_step: 9000, loss: 2.2734375\n",
      "ep: 0, global_step: 9100, loss: 2.357421875\n",
      "ep: 0, global_step: 9200, loss: 2.216796875\n",
      "ep: 0, global_step: 9300, loss: 2.115234375\n",
      "ep: 0, global_step: 9400, loss: 2.220703125\n",
      "ep: 0, global_step: 9500, loss: 2.439453125\n",
      "ep: 0, global_step: 9600, loss: 2.345703125\n",
      "ep: 0, global_step: 9700, loss: 2.44140625\n",
      "ep: 0, global_step: 9800, loss: 2.0078125\n",
      "ep: 0, global_step: 9900, loss: 2.228515625\n",
      "ep: 0, global_step: 10000, loss: 2.240234375\n",
      "ep: 0, global_step: 10100, loss: 2.580078125\n",
      "ep: 0, global_step: 10200, loss: 2.482421875\n",
      "ep: 0, global_step: 10300, loss: 2.095703125\n",
      "ep: 0, global_step: 10400, loss: 2.33984375\n",
      "ep: 0, global_step: 10500, loss: 2.23046875\n",
      "ep: 0, global_step: 10600, loss: 2.25390625\n",
      "ep: 0, global_step: 10700, loss: 2.4140625\n",
      "ep: 0, global_step: 10800, loss: 2.203125\n",
      "ep: 0, global_step: 10900, loss: 2.26171875\n",
      "ep: 0, global_step: 11000, loss: 2.16796875\n",
      "ep: 0, global_step: 11100, loss: 1.8818359375\n",
      "ep: 0, global_step: 11200, loss: 2.025390625\n",
      "ep: 0, global_step: 11300, loss: 2.037109375\n",
      "ep: 0, global_step: 11400, loss: 1.931640625\n",
      "ep: 0, global_step: 11500, loss: 2.46875\n",
      "ep: 0, global_step: 11600, loss: 2.208984375\n",
      "ep: 0, global_step: 11700, loss: 2.33984375\n",
      "ep: 0, global_step: 11800, loss: 2.455078125\n",
      "ep: 0, global_step: 11900, loss: 2.0\n",
      "ep: 0, global_step: 12000, loss: 2.5234375\n",
      "ep: 0, global_step: 12100, loss: 2.01171875\n",
      "ep: 0, global_step: 12200, loss: 2.6484375\n",
      "ep: 0, global_step: 12300, loss: 1.9345703125\n",
      "ep: 0, global_step: 12400, loss: 2.38671875\n",
      "ep: 0, global_step: 12500, loss: 2.65625\n",
      "ep: 0, global_step: 12600, loss: 2.33203125\n",
      "ep: 0, global_step: 12700, loss: 2.431640625\n",
      "ep: 0, global_step: 12800, loss: 2.267578125\n",
      "ep: 0, global_step: 12900, loss: 2.84375\n",
      "ep: 0, global_step: 13000, loss: 2.080078125\n",
      "ep: 0, global_step: 13100, loss: 2.5\n",
      "ep: 0, global_step: 13200, loss: 2.427734375\n",
      "ep: 0, global_step: 13300, loss: 2.318359375\n",
      "ep: 0, global_step: 13400, loss: 2.142578125\n",
      "ep: 0, global_step: 13500, loss: 2.376953125\n",
      "ep: 0, global_step: 13600, loss: 2.12109375\n",
      "ep: 0, global_step: 13700, loss: 2.1640625\n",
      "ep: 0, global_step: 13800, loss: 2.1015625\n",
      "ep: 0, global_step: 13900, loss: 2.1953125\n",
      "ep: 0, global_step: 14000, loss: 2.240234375\n",
      "ep: 0, global_step: 14100, loss: 2.451171875\n",
      "ep: 0, global_step: 14200, loss: 2.412109375\n",
      "ep: 0, global_step: 14300, loss: 2.0234375\n",
      "ep: 0, global_step: 14400, loss: 1.87109375\n",
      "ep: 0, global_step: 14500, loss: 2.177734375\n",
      "ep: 0, global_step: 14600, loss: 2.302734375\n",
      "ep: 0, global_step: 14700, loss: 2.45703125\n",
      "ep: 0, global_step: 14800, loss: 2.072265625\n",
      "ep: 0, global_step: 14900, loss: 1.9228515625\n",
      "ep: 0, global_step: 15000, loss: 2.0859375\n",
      "ep: 0, global_step: 15100, loss: 2.271484375\n",
      "ep: 0, global_step: 15200, loss: 2.265625\n",
      "ep: 0, global_step: 15300, loss: 1.873046875\n",
      "ep: 0, global_step: 15400, loss: 1.884765625\n",
      "ep: 0, global_step: 15500, loss: 2.345703125\n",
      "ep: 0, global_step: 15600, loss: 2.068359375\n",
      "ep: 0, global_step: 15700, loss: 2.353515625\n",
      "ep: 0, global_step: 15800, loss: 2.236328125\n",
      "ep: 0, global_step: 15900, loss: 2.302734375\n",
      "ep: 0, global_step: 16000, loss: 2.09765625\n",
      "ep: 0, global_step: 16100, loss: 2.396484375\n",
      "ep: 0, global_step: 16200, loss: 2.677734375\n",
      "ep: 0, global_step: 16300, loss: 2.162109375\n",
      "ep: 0, global_step: 16400, loss: 2.234375\n",
      "ep: 0, global_step: 16500, loss: 2.78125\n",
      "ep: 0, global_step: 16600, loss: 2.248046875\n",
      "ep: 0, global_step: 16700, loss: 2.68359375\n",
      "ep: 0, global_step: 16800, loss: 2.15625\n",
      "ep: 0, global_step: 16900, loss: 2.529296875\n",
      "ep: 0, global_step: 17000, loss: 2.494140625\n",
      "ep: 0, global_step: 17100, loss: 2.0390625\n",
      "ep: 0, global_step: 17200, loss: 1.98828125\n",
      "ep: 0, global_step: 17300, loss: 2.314453125\n",
      "ep: 0, global_step: 17400, loss: 2.345703125\n",
      "ep: 0, global_step: 17500, loss: 2.02734375\n",
      "ep: 0, global_step: 18000, loss: 2.19921875\n",
      "ep: 0, global_step: 18100, loss: 2.107421875\n",
      "ep: 0, global_step: 18200, loss: 2.38671875\n",
      "ep: 0, global_step: 18300, loss: 2.55859375\n",
      "ep: 0, global_step: 18400, loss: 2.224609375\n",
      "ep: 0, global_step: 18500, loss: 2.18359375\n",
      "ep: 0, global_step: 18600, loss: 2.638671875\n",
      "ep: 0, global_step: 18700, loss: 2.525390625\n",
      "ep: 0, global_step: 18800, loss: 1.845703125\n",
      "ep: 0, global_step: 18900, loss: 2.3359375\n",
      "ep: 0, global_step: 19000, loss: 2.453125\n",
      "ep: 0, global_step: 19100, loss: 2.177734375\n",
      "ep: 0, global_step: 19200, loss: 2.2109375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, global_step: 19300, loss: 1.978515625\n",
      "ep: 0, global_step: 19400, loss: 2.37890625\n",
      "ep: 0, global_step: 19500, loss: 2.412109375\n",
      "ep: 0, global_step: 19600, loss: 2.439453125\n",
      "ep: 0, global_step: 19700, loss: 2.1171875\n",
      "ep: 0, global_step: 19800, loss: 2.357421875\n",
      "ep: 0, global_step: 19900, loss: 2.322265625\n",
      "ep: 0, global_step: 20000, loss: 2.1875\n",
      "ep: 0, global_step: 20100, loss: 2.482421875\n",
      "ep: 0, global_step: 20200, loss: 1.974609375\n",
      "ep: 0, global_step: 20300, loss: 2.59765625\n",
      "ep: 0, global_step: 20400, loss: 1.654296875\n",
      "ep: 0, global_step: 20500, loss: 2.291015625\n",
      "ep: 0, global_step: 20600, loss: 2.216796875\n",
      "ep: 0, global_step: 20700, loss: 1.86328125\n",
      "ep: 0, global_step: 20800, loss: 2.580078125\n",
      "ep: 0, global_step: 20900, loss: 2.10546875\n",
      "ep: 0, global_step: 21000, loss: 2.4765625\n",
      "ep: 0, global_step: 21100, loss: 2.271484375\n",
      "ep: 0, global_step: 21200, loss: 2.294921875\n",
      "ep: 0, global_step: 21300, loss: 2.10546875\n",
      "ep: 0, global_step: 21400, loss: 2.125\n",
      "ep: 0, global_step: 21500, loss: 2.46484375\n",
      "ep: 0, global_step: 21600, loss: 2.373046875\n",
      "ep: 0, global_step: 21700, loss: 2.4140625\n",
      "ep: 0, global_step: 21800, loss: 2.60546875\n",
      "ep: 0, global_step: 21900, loss: 2.44921875\n",
      "ep: 0, global_step: 22000, loss: 2.244140625\n",
      "ep: 0, global_step: 22100, loss: 2.0625\n",
      "ep: 0, global_step: 22200, loss: 1.7216796875\n",
      "ep: 0, global_step: 22300, loss: 2.287109375\n",
      "ep: 0, global_step: 22400, loss: 2.1015625\n",
      "ep: 0, global_step: 22500, loss: 2.025390625\n",
      "ep: 0, global_step: 22600, loss: 2.251953125\n",
      "ep: 0, global_step: 22700, loss: 1.919921875\n",
      "ep: 0, global_step: 22800, loss: 2.31640625\n",
      "ep: 0, global_step: 22900, loss: 2.3125\n",
      "ep: 0, global_step: 23000, loss: 2.732421875\n",
      "ep: 0, global_step: 23100, loss: 2.53515625\n",
      "ep: 0, global_step: 23200, loss: 2.30078125\n",
      "ep: 0, global_step: 23300, loss: 2.51953125\n",
      "ep: 0, global_step: 23400, loss: 2.28515625\n",
      "ep: 0, global_step: 23500, loss: 2.390625\n",
      "ep: 0, global_step: 23600, loss: 2.4921875\n",
      "ep: 0, global_step: 23700, loss: 2.541015625\n",
      "ep: 0, global_step: 23800, loss: 2.12109375\n",
      "ep: 0, global_step: 23900, loss: 2.294921875\n",
      "ep: 0, global_step: 24000, loss: 2.27734375\n",
      "ep: 0, global_step: 24100, loss: 1.85546875\n",
      "ep: 0, global_step: 24200, loss: 2.3515625\n",
      "ep: 0, global_step: 24300, loss: 2.109375\n",
      "ep: 0, global_step: 24400, loss: 2.23828125\n",
      "ep: 0, global_step: 24500, loss: 2.26171875\n",
      "ep: 0, global_step: 24600, loss: 2.328125\n",
      "ep: 0, global_step: 24700, loss: 2.30859375\n",
      "ep: 0, global_step: 24800, loss: 2.2265625\n",
      "ep: 0, global_step: 24900, loss: 2.19921875\n",
      "ep: 0, global_step: 25000, loss: 1.7099609375\n",
      "ep: 0, global_step: 25100, loss: 2.09765625\n",
      "ep: 0, global_step: 25200, loss: 2.365234375\n",
      "ep: 0, global_step: 25300, loss: 2.373046875\n",
      "ep: 0, global_step: 25400, loss: 2.087890625\n",
      "ep: 0, global_step: 25500, loss: 2.203125\n",
      "ep: 0, global_step: 25600, loss: 2.544921875\n",
      "ep: 0, global_step: 25700, loss: 2.25390625\n",
      "ep: 0, global_step: 25800, loss: 2.244140625\n",
      "ep: 0, global_step: 25900, loss: 2.12109375\n",
      "ep: 0, global_step: 26000, loss: 2.126953125\n",
      "ep: 0, global_step: 26100, loss: 2.634765625\n",
      "ep: 0, global_step: 26200, loss: 1.9365234375\n",
      "ep: 0, global_step: 26300, loss: 2.25390625\n",
      "ep: 0, global_step: 26400, loss: 2.361328125\n",
      "ep: 0, global_step: 26500, loss: 2.216796875\n",
      "ep: 0, global_step: 26600, loss: 1.9736328125\n",
      "ep: 0, global_step: 26700, loss: 2.27734375\n",
      "ep: 0, global_step: 26800, loss: 2.173828125\n",
      "ep: 0, global_step: 26900, loss: 2.5234375\n",
      "ep: 0, global_step: 27000, loss: 2.33203125\n",
      "ep: 0, global_step: 27100, loss: 2.15234375\n",
      "ep: 0, global_step: 27200, loss: 2.349609375\n",
      "ep: 0, global_step: 27300, loss: 2.14453125\n",
      "ep: 0, global_step: 27400, loss: 1.9189453125\n",
      "ep: 0, global_step: 27500, loss: 2.220703125\n",
      "ep: 0, global_step: 27600, loss: 2.142578125\n",
      "ep: 0, global_step: 27700, loss: 2.443359375\n",
      "ep: 0, global_step: 27800, loss: 1.849609375\n",
      "ep: 0, global_step: 27900, loss: 2.41796875\n",
      "ep: 0, global_step: 28000, loss: 2.453125\n",
      "ep: 0, global_step: 28100, loss: 2.45703125\n",
      "ep: 0, global_step: 28200, loss: 2.595703125\n",
      "ep: 0, global_step: 28300, loss: 1.771484375\n",
      "ep: 0, global_step: 28400, loss: 1.97265625\n",
      "ep: 0, global_step: 28500, loss: 2.623046875\n",
      "ep: 0, global_step: 28600, loss: 2.16796875\n",
      "ep: 0, global_step: 28700, loss: 2.28125\n",
      "ep: 0, global_step: 28800, loss: 2.341796875\n",
      "ep: 0, global_step: 28900, loss: 2.41796875\n",
      "ep: 0, global_step: 29000, loss: 2.125\n",
      "ep: 0, global_step: 29100, loss: 2.3046875\n",
      "ep: 0, global_step: 29200, loss: 2.099609375\n",
      "ep: 0, global_step: 29300, loss: 2.333984375\n",
      "ep: 0, global_step: 29400, loss: 2.33203125\n",
      "ep: 0, global_step: 29500, loss: 2.2265625\n",
      "ep: 0, global_step: 29600, loss: 2.337890625\n",
      "ep: 0, global_step: 29700, loss: 2.7734375\n",
      "ep: 0, global_step: 29800, loss: 2.4296875\n",
      "ep: 0, global_step: 29900, loss: 2.421875\n",
      "ep: 0, global_step: 30000, loss: 2.67578125\n",
      "ep: 0, global_step: 30100, loss: 1.8056640625\n",
      "ep: 0, global_step: 30200, loss: 2.201171875\n",
      "ep: 0, global_step: 30300, loss: 2.2265625\n",
      "ep: 0, global_step: 30400, loss: 2.546875\n",
      "ep: 0, global_step: 30500, loss: 2.583984375\n",
      "ep: 0, global_step: 30600, loss: 2.20703125\n",
      "ep: 0, global_step: 30700, loss: 2.359375\n",
      "ep: 0, global_step: 30800, loss: 1.931640625\n",
      "ep: 0, global_step: 30900, loss: 2.40234375\n",
      "ep: 0, global_step: 31000, loss: 1.9833984375\n",
      "ep: 0, global_step: 31100, loss: 2.287109375\n",
      "ep: 0, global_step: 31200, loss: 2.337890625\n",
      "ep: 0, global_step: 31300, loss: 2.28515625\n",
      "ep: 0, global_step: 31400, loss: 2.158203125\n",
      "ep: 0, global_step: 31500, loss: 2.625\n",
      "ep: 0, global_step: 31600, loss: 2.0234375\n",
      "ep: 0, global_step: 31700, loss: 2.51953125\n",
      "ep: 0, global_step: 31800, loss: 2.5\n",
      "ep: 0, global_step: 31900, loss: 2.09375\n",
      "ep: 0, global_step: 32000, loss: 1.794921875\n",
      "ep: 0, global_step: 32100, loss: 2.201171875\n",
      "ep: 0, global_step: 32200, loss: 2.078125\n",
      "ep: 0, global_step: 32300, loss: 1.9873046875\n",
      "ep: 0, global_step: 32400, loss: 2.234375\n",
      "ep: 0, global_step: 32500, loss: 2.5390625\n",
      "ep: 0, global_step: 32600, loss: 1.6708984375\n",
      "ep: 0, global_step: 32700, loss: 2.505859375\n",
      "ep: 0, global_step: 32800, loss: 2.244140625\n",
      "ep: 0, global_step: 32900, loss: 2.337890625\n",
      "ep: 0, global_step: 33000, loss: 2.185546875\n",
      "ep: 0, global_step: 33100, loss: 2.224609375\n",
      "ep: 0, global_step: 33200, loss: 2.447265625\n",
      "ep: 0, global_step: 33300, loss: 2.10546875\n",
      "ep: 0, global_step: 33400, loss: 2.18359375\n",
      "ep: 0, global_step: 33500, loss: 2.162109375\n",
      "ep: 0, global_step: 33600, loss: 2.2890625\n",
      "ep: 0, global_step: 33700, loss: 2.283203125\n",
      "ep: 0, global_step: 33800, loss: 2.078125\n",
      "ep: 0, global_step: 33900, loss: 2.2421875\n",
      "ep: 0, global_step: 34000, loss: 2.169921875\n",
      "ep: 0, global_step: 34100, loss: 2.15625\n",
      "ep: 0, global_step: 34200, loss: 2.505859375\n",
      "ep: 0, global_step: 34300, loss: 2.09375\n",
      "ep: 0, global_step: 34400, loss: 2.201171875\n",
      "ep: 0, global_step: 34500, loss: 2.47265625\n",
      "ep: 0, global_step: 34600, loss: 2.744140625\n",
      "ep: 0, global_step: 34700, loss: 2.193359375\n",
      "ep: 0, global_step: 34800, loss: 2.203125\n",
      "ep: 0, global_step: 34900, loss: 2.09375\n",
      "ep: 0, global_step: 35000, loss: 2.173828125\n",
      "ep: 0, global_step: 35100, loss: 2.322265625\n",
      "ep: 0, global_step: 35200, loss: 2.341796875\n",
      "ep: 0, global_step: 35300, loss: 2.46484375\n",
      "ep: 0, global_step: 35400, loss: 2.3671875\n",
      "ep: 0, global_step: 35500, loss: 2.384765625\n",
      "ep: 0, global_step: 35600, loss: 2.318359375\n",
      "ep: 0, global_step: 35700, loss: 2.8046875\n",
      "ep: 0, global_step: 35800, loss: 2.3671875\n",
      "ep: 0, global_step: 35900, loss: 2.30078125\n",
      "ep: 0, global_step: 36000, loss: 2.365234375\n",
      "ep: 0, global_step: 36100, loss: 2.431640625\n",
      "ep: 0, global_step: 36200, loss: 2.287109375\n",
      "ep: 0, global_step: 36300, loss: 2.068359375\n",
      "ep: 0, global_step: 36400, loss: 2.169921875\n",
      "ep: 0, global_step: 36500, loss: 2.150390625\n",
      "ep: 0, global_step: 36600, loss: 1.9443359375\n",
      "ep: 0, global_step: 36700, loss: 2.02734375\n",
      "ep: 0, global_step: 36800, loss: 2.224609375\n",
      "ep: 0, global_step: 36900, loss: 2.3125\n",
      "ep: 0, global_step: 37000, loss: 1.7080078125\n",
      "ep: 0, global_step: 37100, loss: 2.337890625\n",
      "ep: 0, global_step: 37200, loss: 2.478515625\n",
      "ep: 0, global_step: 37300, loss: 2.595703125\n",
      "ep: 0, global_step: 37400, loss: 2.244140625\n",
      "ep: 0, global_step: 37500, loss: 2.291015625\n",
      "ep: 0, global_step: 37600, loss: 2.2734375\n",
      "ep: 0, global_step: 37700, loss: 2.1484375\n",
      "ep: 0, global_step: 37800, loss: 2.333984375\n",
      "ep: 0, global_step: 37900, loss: 2.1015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, global_step: 38000, loss: 2.11328125\n",
      "ep: 0, global_step: 38100, loss: 2.029296875\n",
      "ep: 0, global_step: 38200, loss: 2.421875\n",
      "ep: 0, global_step: 38300, loss: 2.5703125\n",
      "ep: 0, global_step: 38400, loss: 2.203125\n",
      "ep: 0, global_step: 38500, loss: 2.41015625\n",
      "ep: 0, global_step: 38600, loss: 2.115234375\n",
      "ep: 0, global_step: 38700, loss: 2.541015625\n",
      "ep: 0, global_step: 38800, loss: 2.31640625\n",
      "ep: 0, global_step: 38900, loss: 2.6796875\n",
      "ep: 0, global_step: 39000, loss: 2.46484375\n",
      "ep: 0, global_step: 39100, loss: 2.09765625\n",
      "ep: 0, global_step: 39200, loss: 2.326171875\n",
      "ep: 0, global_step: 39300, loss: 2.46875\n",
      "ep: 0, global_step: 39400, loss: 2.505859375\n",
      "ep: 0, global_step: 39500, loss: 2.369140625\n",
      "ep: 0, global_step: 39600, loss: 2.373046875\n",
      "ep: 0, global_step: 39700, loss: 1.7421875\n",
      "ep: 0, global_step: 39800, loss: 2.0703125\n",
      "ep: 0, global_step: 39900, loss: 1.83203125\n",
      "ep: 0, global_step: 40000, loss: 1.8076171875\n",
      "ep: 0, global_step: 40100, loss: 2.294921875\n",
      "ep: 0, global_step: 40200, loss: 2.1328125\n",
      "ep: 0, global_step: 40300, loss: 2.564453125\n",
      "ep: 0, global_step: 40400, loss: 2.04296875\n",
      "ep: 0, global_step: 40500, loss: 2.48828125\n",
      "ep: 0, global_step: 40600, loss: 2.048828125\n",
      "ep: 0, global_step: 40700, loss: 2.46875\n",
      "ep: 0, global_step: 40800, loss: 2.517578125\n",
      "ep: 0, global_step: 40900, loss: 2.39453125\n",
      "ep: 0, global_step: 41000, loss: 2.482421875\n",
      "ep: 0, global_step: 41100, loss: 2.1171875\n",
      "ep: 0, global_step: 41200, loss: 2.404296875\n",
      "ep: 0, global_step: 41300, loss: 2.27734375\n",
      "ep: 0, global_step: 41400, loss: 2.32421875\n",
      "ep: 0, global_step: 41500, loss: 2.146484375\n",
      "ep: 0, global_step: 41600, loss: 1.978515625\n",
      "ep: 0, global_step: 41700, loss: 1.775390625\n",
      "ep: 0, global_step: 41800, loss: 2.1796875\n",
      "ep: 0, global_step: 41900, loss: 2.314453125\n",
      "ep: 0, global_step: 42000, loss: 2.546875\n",
      "ep: 0, global_step: 42100, loss: 2.599609375\n",
      "ep: 0, global_step: 42200, loss: 2.294921875\n",
      "ep: 0, global_step: 42300, loss: 2.056640625\n",
      "ep: 0, global_step: 42400, loss: 2.046875\n",
      "ep: 0, global_step: 42500, loss: 2.0078125\n",
      "ep: 0, global_step: 42600, loss: 2.31640625\n",
      "ep: 0, global_step: 42700, loss: 1.9140625\n",
      "ep: 0, global_step: 42800, loss: 2.408203125\n",
      "ep: 0, global_step: 42900, loss: 1.9794921875\n",
      "ep: 0, global_step: 43000, loss: 2.390625\n",
      "ep: 0, global_step: 43100, loss: 2.654296875\n",
      "ep: 0, global_step: 43200, loss: 2.2890625\n",
      "ep: 0, global_step: 43300, loss: 1.798828125\n",
      "ep: 0, global_step: 43400, loss: 2.42578125\n",
      "ep: 0, global_step: 43500, loss: 1.9951171875\n",
      "ep: 0, global_step: 43600, loss: 2.21875\n",
      "ep: 0, global_step: 43700, loss: 2.326171875\n",
      "ep: 0, global_step: 43800, loss: 2.076171875\n",
      "ep: 0, global_step: 43900, loss: 2.36328125\n",
      "ep: 0, global_step: 44000, loss: 2.443359375\n",
      "ep: 0, global_step: 44100, loss: 2.30859375\n",
      "ep: 0, global_step: 44200, loss: 2.466796875\n",
      "ep: 0, global_step: 44300, loss: 2.6796875\n",
      "ep: 0, global_step: 44400, loss: 2.1484375\n",
      "ep: 0, global_step: 44500, loss: 2.640625\n",
      "ep: 0, global_step: 44600, loss: 2.5\n",
      "ep: 0, global_step: 44700, loss: 2.08984375\n",
      "ep: 0, global_step: 44800, loss: 1.9912109375\n",
      "ep: 0, global_step: 44900, loss: 1.966796875\n",
      "ep: 0, global_step: 45000, loss: 2.486328125\n",
      "ep: 0, global_step: 45100, loss: 2.5703125\n",
      "ep: 0, global_step: 45200, loss: 2.0625\n",
      "ep: 0, global_step: 45300, loss: 2.3203125\n",
      "ep: 0, global_step: 45400, loss: 2.177734375\n",
      "ep: 0, global_step: 45500, loss: 2.234375\n",
      "ep: 0, global_step: 45600, loss: 2.076171875\n",
      "ep: 0, global_step: 45700, loss: 2.259765625\n",
      "ep: 0, global_step: 45800, loss: 2.4296875\n",
      "ep: 0, global_step: 45900, loss: 2.169921875\n",
      "ep: 0, global_step: 46000, loss: 2.4609375\n",
      "ep: 0, global_step: 46100, loss: 2.171875\n",
      "ep: 0, global_step: 46200, loss: 2.626953125\n",
      "ep: 0, global_step: 46300, loss: 2.271484375\n",
      "ep: 0, global_step: 46400, loss: 2.1796875\n",
      "ep: 0, global_step: 46500, loss: 2.185546875\n",
      "ep: 0, global_step: 46600, loss: 2.677734375\n",
      "ep: 0, global_step: 46700, loss: 2.43359375\n",
      "ep: 0, global_step: 46800, loss: 2.859375\n",
      "ep: 0, global_step: 46900, loss: 2.7421875\n",
      "ep: 0, global_step: 47000, loss: 2.4609375\n",
      "ep: 0, global_step: 47100, loss: 2.349609375\n",
      "ep: 0, global_step: 47200, loss: 2.353515625\n",
      "ep: 0, global_step: 47300, loss: 1.8837890625\n",
      "ep: 0, global_step: 47400, loss: 2.234375\n",
      "ep: 0, global_step: 47500, loss: 2.162109375\n",
      "ep: 0, global_step: 47600, loss: 2.357421875\n",
      "ep: 0, global_step: 47700, loss: 2.10546875\n",
      "ep: 0, global_step: 47800, loss: 2.408203125\n",
      "ep: 0, global_step: 47900, loss: 2.337890625\n",
      "ep: 0, global_step: 48000, loss: 2.189453125\n",
      "ep: 0, global_step: 48100, loss: 2.30078125\n",
      "ep: 0, global_step: 48200, loss: 2.33203125\n",
      "ep: 0, global_step: 48300, loss: 1.7958984375\n",
      "ep: 0, global_step: 48400, loss: 1.912109375\n",
      "ep: 0, global_step: 48500, loss: 2.091796875\n",
      "ep: 0, global_step: 48600, loss: 2.044921875\n",
      "ep: 0, global_step: 48700, loss: 2.23046875\n",
      "ep: 0, global_step: 48800, loss: 2.140625\n",
      "ep: 0, global_step: 48900, loss: 2.115234375\n",
      "ep: 0, global_step: 49000, loss: 2.244140625\n",
      "ep: 0, global_step: 49100, loss: 2.287109375\n",
      "ep: 0, global_step: 49200, loss: 2.08203125\n",
      "ep: 0, global_step: 49300, loss: 2.39453125\n",
      "ep: 0, global_step: 49400, loss: 2.6171875\n",
      "ep: 0, global_step: 49500, loss: 2.302734375\n",
      "ep: 0, global_step: 49600, loss: 2.47265625\n",
      "ep: 0, global_step: 49700, loss: 2.466796875\n",
      "ep: 0, global_step: 49800, loss: 2.068359375\n",
      "ep: 0, global_step: 49900, loss: 2.359375\n",
      "ep: 0, global_step: 50000, loss: 2.060546875\n",
      "ep: 0, global_step: 50100, loss: 2.501953125\n",
      "ep: 0, global_step: 50200, loss: 2.072265625\n",
      "ep: 0, global_step: 50300, loss: 2.298828125\n",
      "ep: 0, global_step: 50400, loss: 2.517578125\n",
      "ep: 0, global_step: 50500, loss: 1.9931640625\n",
      "ep: 0, global_step: 50600, loss: 2.5859375\n",
      "ep: 0, global_step: 50700, loss: 2.203125\n",
      "ep: 0, global_step: 50800, loss: 2.18359375\n",
      "ep: 0, global_step: 50900, loss: 2.4296875\n",
      "ep: 0, global_step: 51000, loss: 2.26953125\n",
      "ep: 0, global_step: 51100, loss: 1.994140625\n",
      "ep: 0, global_step: 51200, loss: 2.33984375\n",
      "ep: 0, global_step: 51300, loss: 2.146484375\n",
      "ep: 0, global_step: 51400, loss: 2.361328125\n",
      "ep: 0, global_step: 51500, loss: 2.185546875\n",
      "ep: 0, global_step: 51600, loss: 2.55859375\n",
      "ep: 0, global_step: 51700, loss: 2.25390625\n",
      "ep: 0, global_step: 51800, loss: 2.19140625\n",
      "ep: 0, global_step: 51900, loss: 2.546875\n",
      "ep: 0, global_step: 52000, loss: 2.228515625\n",
      "ep: 0, global_step: 52100, loss: 2.546875\n",
      "ep: 0, global_step: 52200, loss: 1.75390625\n",
      "ep: 0, global_step: 52300, loss: 2.5625\n",
      "ep: 0, global_step: 52400, loss: 2.314453125\n",
      "ep: 0, global_step: 52500, loss: 2.376953125\n",
      "ep: 0, global_step: 52600, loss: 1.990234375\n",
      "ep: 0, global_step: 52700, loss: 2.314453125\n",
      "ep: 0, global_step: 52800, loss: 2.408203125\n",
      "ep: 0, global_step: 52900, loss: 2.28125\n",
      "ep: 0, global_step: 53000, loss: 2.43359375\n",
      "ep: 0, global_step: 53100, loss: 2.4921875\n",
      "ep: 0, global_step: 53200, loss: 2.490234375\n",
      "ep: 0, global_step: 53300, loss: 2.138671875\n",
      "ep: 0, global_step: 53400, loss: 1.474609375\n",
      "ep: 0, global_step: 53500, loss: 2.10546875\n",
      "ep: 0, global_step: 53600, loss: 2.4765625\n",
      "ep: 0, global_step: 53700, loss: 2.24609375\n",
      "ep: 0, global_step: 53800, loss: 1.935546875\n",
      "ep: 0, global_step: 53900, loss: 2.478515625\n",
      "ep: 0, global_step: 54000, loss: 1.90234375\n",
      "ep: 0, global_step: 54100, loss: 2.33984375\n",
      "ep: 0, global_step: 54200, loss: 2.294921875\n",
      "ep: 0, global_step: 54300, loss: 1.9365234375\n",
      "ep: 0, global_step: 54400, loss: 2.533203125\n",
      "ep: 0, global_step: 54500, loss: 1.9228515625\n",
      "ep: 0, global_step: 54600, loss: 2.34765625\n",
      "ep: 0, global_step: 54700, loss: 1.99609375\n",
      "ep: 0, global_step: 54800, loss: 2.248046875\n",
      "ep: 0, global_step: 54900, loss: 2.169921875\n",
      "ep: 0, global_step: 55000, loss: 2.4453125\n",
      "ep: 0, global_step: 55100, loss: 1.9755859375\n",
      "ep: 0, global_step: 55200, loss: 2.3515625\n",
      "ep: 0, global_step: 55300, loss: 2.40625\n",
      "ep: 0, global_step: 55400, loss: 2.080078125\n",
      "ep: 0, global_step: 55500, loss: 2.48828125\n",
      "ep: 0, global_step: 55600, loss: 2.349609375\n",
      "ep: 0, global_step: 55700, loss: 1.8896484375\n",
      "ep: 0, global_step: 55800, loss: 2.447265625\n",
      "ep: 0, global_step: 55900, loss: 2.412109375\n",
      "ep: 0, global_step: 56000, loss: 2.169921875\n",
      "ep: 0, global_step: 56100, loss: 2.54296875\n",
      "ep: 0, global_step: 56200, loss: 2.173828125\n",
      "ep: 0, global_step: 56300, loss: 2.474609375\n",
      "ep: 0, global_step: 56400, loss: 2.03515625\n",
      "ep: 0, global_step: 56500, loss: 2.283203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, global_step: 56600, loss: 2.224609375\n",
      "ep: 0, global_step: 56700, loss: 2.359375\n",
      "ep: 0, global_step: 56800, loss: 2.455078125\n",
      "ep: 0, global_step: 56900, loss: 2.509765625\n",
      "ep: 0, global_step: 57000, loss: 2.404296875\n",
      "ep: 0, global_step: 57100, loss: 2.458984375\n",
      "ep: 0, global_step: 57200, loss: 2.060546875\n",
      "ep: 0, global_step: 57300, loss: 2.2734375\n",
      "ep: 0, global_step: 57400, loss: 2.392578125\n",
      "ep: 0, global_step: 57500, loss: 2.134765625\n",
      "ep: 0, global_step: 57600, loss: 2.357421875\n",
      "ep: 0, global_step: 57700, loss: 2.298828125\n",
      "ep: 0, global_step: 57800, loss: 2.048828125\n",
      "ep: 0, global_step: 57900, loss: 2.53125\n",
      "ep: 0, global_step: 58000, loss: 2.23828125\n",
      "ep: 0, global_step: 58100, loss: 2.375\n",
      "ep: 0, global_step: 58200, loss: 1.916015625\n",
      "ep: 0, global_step: 58300, loss: 2.00390625\n",
      "ep: 0, global_step: 58400, loss: 2.548828125\n",
      "ep: 0, global_step: 58500, loss: 2.302734375\n",
      "ep: 0, global_step: 58600, loss: 2.107421875\n",
      "ep: 0, global_step: 58700, loss: 2.552734375\n",
      "ep: 0, global_step: 58800, loss: 2.0390625\n",
      "ep: 0, global_step: 58900, loss: 2.255859375\n",
      "ep: 0, global_step: 59000, loss: 2.060546875\n",
      "ep: 0, global_step: 59100, loss: 1.9794921875\n",
      "ep: 0, global_step: 59200, loss: 2.064453125\n",
      "ep: 0, global_step: 59300, loss: 2.369140625\n",
      "ep: 0, global_step: 59400, loss: 2.357421875\n",
      "ep: 0, global_step: 59500, loss: 1.8994140625\n",
      "ep: 0, global_step: 59600, loss: 2.34375\n",
      "ep: 0, global_step: 59700, loss: 2.060546875\n",
      "ep: 0, global_step: 59800, loss: 2.51171875\n",
      "ep: 0, global_step: 59900, loss: 2.478515625\n",
      "ep: 0, global_step: 60000, loss: 2.3671875\n",
      "ep: 0, global_step: 60100, loss: 2.400390625\n",
      "ep: 0, global_step: 60200, loss: 2.291015625\n",
      "ep: 0, global_step: 60300, loss: 2.595703125\n",
      "ep: 0, global_step: 60400, loss: 2.412109375\n",
      "ep: 0, global_step: 60500, loss: 2.328125\n",
      "ep: 0, global_step: 60600, loss: 2.232421875\n",
      "ep: 0, global_step: 60700, loss: 2.193359375\n",
      "ep: 0, global_step: 60800, loss: 2.375\n",
      "ep: 0, global_step: 60900, loss: 2.126953125\n",
      "ep: 0, global_step: 61000, loss: 2.279296875\n",
      "ep: 0, global_step: 61100, loss: 2.138671875\n",
      "ep: 0, global_step: 61200, loss: 2.455078125\n",
      "ep: 0, global_step: 61300, loss: 2.677734375\n",
      "ep: 0, global_step: 61400, loss: 2.296875\n",
      "ep: 0, global_step: 61500, loss: 2.32421875\n",
      "ep: 0, global_step: 61600, loss: 2.544921875\n",
      "ep: 0, global_step: 61700, loss: 1.908203125\n",
      "ep: 0, global_step: 61800, loss: 2.123046875\n",
      "ep: 0, global_step: 61900, loss: 2.083984375\n",
      "ep: 0, global_step: 62000, loss: 2.0859375\n",
      "ep: 0, global_step: 62100, loss: 2.4921875\n",
      "ep: 0, global_step: 62200, loss: 2.333984375\n",
      "ep: 0, global_step: 62300, loss: 2.09375\n",
      "ep: 0, global_step: 62400, loss: 2.0703125\n",
      "ep: 0, global_step: 62500, loss: 2.078125\n",
      "ep: 0, global_step: 62600, loss: 2.328125\n",
      "ep: 0, global_step: 62700, loss: 2.140625\n",
      "ep: 0, global_step: 62800, loss: 2.107421875\n",
      "ep: 0, global_step: 62900, loss: 2.08203125\n",
      "ep: 0, global_step: 63000, loss: 2.32421875\n",
      "ep: 0, global_step: 63100, loss: 2.044921875\n",
      "ep: 0, global_step: 63200, loss: 2.20703125\n",
      "ep: 0, global_step: 63300, loss: 2.23828125\n",
      "ep: 0, global_step: 63400, loss: 2.65625\n",
      "ep: 0, global_step: 63500, loss: 2.4765625\n",
      "ep: 0, global_step: 63600, loss: 1.779296875\n",
      "ep: 0, global_step: 63700, loss: 2.296875\n",
      "ep: 0, global_step: 63800, loss: 2.67578125\n",
      "ep: 0, global_step: 63900, loss: 2.49609375\n",
      "ep: 0, global_step: 64000, loss: 2.07421875\n",
      "ep: 0, global_step: 64100, loss: 2.05078125\n",
      "ep: 0, global_step: 64200, loss: 1.6689453125\n",
      "ep: 0, global_step: 64300, loss: 1.939453125\n",
      "ep: 0, global_step: 64400, loss: 2.017578125\n",
      "ep: 0, global_step: 64500, loss: 2.3984375\n",
      "ep: 0, global_step: 64600, loss: 2.087890625\n",
      "ep: 0, global_step: 64700, loss: 2.5\n",
      "ep: 0, global_step: 64800, loss: 2.19140625\n",
      "ep: 0, global_step: 64900, loss: 2.375\n",
      "ep: 0, global_step: 65000, loss: 2.369140625\n",
      "ep: 0, global_step: 65100, loss: 1.8203125\n",
      "ep: 0, global_step: 65200, loss: 2.33984375\n",
      "ep: 0, global_step: 65300, loss: 2.0234375\n",
      "ep: 0, global_step: 65400, loss: 2.294921875\n",
      "ep: 0, global_step: 65500, loss: 2.333984375\n",
      "ep: 0, global_step: 65600, loss: 2.037109375\n",
      "ep: 0, global_step: 65700, loss: 1.962890625\n",
      "ep: 0, global_step: 65800, loss: 2.1875\n",
      "ep: 0, global_step: 65900, loss: 2.3671875\n",
      "ep: 0, global_step: 66000, loss: 2.306640625\n",
      "ep: 0, global_step: 66100, loss: 2.62890625\n",
      "ep: 0, global_step: 66200, loss: 2.435546875\n",
      "ep: 0, global_step: 66300, loss: 2.1171875\n",
      "ep: 0, global_step: 66400, loss: 1.90234375\n",
      "ep: 0, global_step: 66500, loss: 2.052734375\n",
      "ep: 0, global_step: 66600, loss: 2.36328125\n",
      "ep: 0, global_step: 66700, loss: 2.423828125\n",
      "ep: 0, global_step: 66800, loss: 2.369140625\n",
      "ep: 0, global_step: 66900, loss: 2.111328125\n",
      "ep: 0, global_step: 67000, loss: 2.283203125\n",
      "ep: 0, global_step: 67100, loss: 2.544921875\n",
      "ep: 0, global_step: 67200, loss: 1.9423828125\n",
      "ep: 0, global_step: 67300, loss: 2.48046875\n",
      "ep: 0, global_step: 67400, loss: 2.392578125\n",
      "ep: 0, global_step: 67500, loss: 2.611328125\n",
      "ep: 0, global_step: 67600, loss: 1.9677734375\n",
      "ep: 0, global_step: 67700, loss: 2.306640625\n",
      "ep: 0, global_step: 67800, loss: 2.205078125\n",
      "ep: 0, global_step: 67900, loss: 2.298828125\n",
      "ep: 0, global_step: 68000, loss: 2.314453125\n",
      "ep: 0, global_step: 68100, loss: 2.697265625\n",
      "ep: 0, global_step: 68200, loss: 2.376953125\n",
      "ep: 0, global_step: 68300, loss: 2.384765625\n",
      "ep: 0, global_step: 68400, loss: 2.42578125\n",
      "ep: 0, global_step: 68500, loss: 2.51953125\n",
      "ep: 0, global_step: 68600, loss: 2.82421875\n",
      "ep: 0, global_step: 68700, loss: 2.60546875\n",
      "ep: 0, global_step: 68800, loss: 2.0234375\n",
      "ep: 0, global_step: 68900, loss: 2.1875\n",
      "ep: 0, global_step: 69000, loss: 2.146484375\n",
      "ep: 0, global_step: 69100, loss: 2.009765625\n",
      "ep: 0, global_step: 69200, loss: 2.359375\n",
      "ep: 0, global_step: 69300, loss: 2.24609375\n",
      "ep: 0, global_step: 69400, loss: 2.388671875\n",
      "ep: 0, global_step: 69500, loss: 1.9970703125\n",
      "ep: 0, global_step: 69600, loss: 2.158203125\n",
      "ep: 0, global_step: 69700, loss: 2.166015625\n",
      "ep: 0, global_step: 69800, loss: 2.7734375\n",
      "ep: 0, global_step: 69900, loss: 2.66015625\n",
      "ep: 0, global_step: 70000, loss: 2.619140625\n",
      "ep: 0, global_step: 70100, loss: 1.8740234375\n",
      "ep: 0, global_step: 70200, loss: 1.6787109375\n",
      "ep: 0, global_step: 70300, loss: 2.09765625\n",
      "ep: 0, global_step: 70400, loss: 2.072265625\n",
      "ep: 0, global_step: 70500, loss: 2.275390625\n",
      "ep: 0, global_step: 70600, loss: 2.11328125\n",
      "ep: 0, global_step: 70700, loss: 2.078125\n",
      "ep: 0, global_step: 70800, loss: 2.134765625\n",
      "ep: 0, global_step: 70900, loss: 2.41015625\n",
      "ep: 0, global_step: 71000, loss: 1.9814453125\n",
      "ep: 0, global_step: 71100, loss: 1.8876953125\n",
      "ep: 0, global_step: 71200, loss: 2.369140625\n",
      "ep: 0, global_step: 71300, loss: 2.12109375\n",
      "ep: 0, global_step: 71400, loss: 2.30859375\n",
      "ep: 0, global_step: 71500, loss: 2.4375\n",
      "ep: 0, global_step: 71600, loss: 2.162109375\n",
      "ep: 0, global_step: 71700, loss: 2.8671875\n",
      "ep: 0, global_step: 71800, loss: 2.078125\n",
      "ep: 0, global_step: 71900, loss: 2.2265625\n",
      "ep: 0, global_step: 72000, loss: 1.849609375\n",
      "ep: 0, global_step: 72100, loss: 2.02734375\n",
      "ep: 0, global_step: 72200, loss: 2.294921875\n",
      "ep: 0, global_step: 72300, loss: 2.302734375\n",
      "ep: 0, global_step: 72400, loss: 2.306640625\n",
      "ep: 0, global_step: 72500, loss: 1.7978515625\n",
      "ep: 0, global_step: 72600, loss: 2.20703125\n",
      "ep: 0, global_step: 72700, loss: 1.7353515625\n",
      "ep: 0, global_step: 72800, loss: 1.8017578125\n",
      "ep: 0, global_step: 72900, loss: 2.283203125\n",
      "ep: 0, global_step: 73000, loss: 2.263671875\n",
      "ep: 0, global_step: 73100, loss: 2.150390625\n",
      "ep: 0, global_step: 73200, loss: 2.375\n",
      "ep: 0, global_step: 73300, loss: 2.263671875\n",
      "ep: 0, global_step: 73400, loss: 2.025390625\n",
      "ep: 0, global_step: 73500, loss: 2.638671875\n",
      "ep: 0, global_step: 73600, loss: 2.09375\n",
      "ep: 0, global_step: 73700, loss: 1.84375\n",
      "ep: 0, global_step: 73800, loss: 1.96875\n",
      "ep: 0, global_step: 73900, loss: 1.765625\n",
      "ep: 0, global_step: 74000, loss: 2.044921875\n",
      "ep: 0, global_step: 74100, loss: 2.548828125\n",
      "ep: 0, global_step: 74200, loss: 2.4140625\n",
      "ep: 0, global_step: 74300, loss: 2.25390625\n",
      "ep: 0, global_step: 74400, loss: 2.052734375\n",
      "ep: 0, global_step: 74500, loss: 2.470703125\n",
      "ep: 0, global_step: 74600, loss: 2.302734375\n",
      "ep: 0, global_step: 74700, loss: 2.40234375\n",
      "ep: 0, global_step: 74800, loss: 2.509765625\n",
      "ep: 0, global_step: 74900, loss: 2.58203125\n",
      "ep: 0, global_step: 75000, loss: 2.53125\n",
      "ep: 0, global_step: 75100, loss: 2.494140625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, global_step: 75200, loss: 1.85546875\n",
      "ep: 0, global_step: 75300, loss: 2.140625\n",
      "ep: 0, global_step: 75400, loss: 2.40625\n",
      "ep: 0, global_step: 75500, loss: 2.126953125\n",
      "ep: 0, global_step: 75600, loss: 2.587890625\n",
      "ep: 0, global_step: 75700, loss: 2.044921875\n",
      "ep: 0, global_step: 75800, loss: 2.20703125\n",
      "ep: 0, global_step: 75900, loss: 1.990234375\n",
      "ep: 0, global_step: 76000, loss: 1.93359375\n",
      "ep: 0, global_step: 76100, loss: 2.23046875\n",
      "ep: 0, global_step: 76200, loss: 2.19921875\n",
      "ep: 0, global_step: 76300, loss: 2.1484375\n",
      "ep: 0, global_step: 76400, loss: 2.38671875\n",
      "ep: 0, global_step: 76500, loss: 2.39453125\n",
      "ep: 0, global_step: 76600, loss: 2.525390625\n",
      "ep: 0, global_step: 76700, loss: 2.359375\n",
      "ep: 0, global_step: 76800, loss: 2.564453125\n",
      "ep: 0, global_step: 76900, loss: 2.494140625\n",
      "ep: 0, global_step: 77000, loss: 2.740234375\n",
      "ep: 0, global_step: 77100, loss: 2.287109375\n",
      "ep: 0, global_step: 77200, loss: 2.275390625\n",
      "ep: 0, global_step: 77300, loss: 1.9267578125\n",
      "ep: 0, global_step: 77400, loss: 2.6171875\n",
      "ep: 0, global_step: 77500, loss: 2.595703125\n",
      "ep: 0, global_step: 77600, loss: 1.9853515625\n",
      "ep: 0, global_step: 77700, loss: 2.419921875\n",
      "ep: 0, global_step: 77800, loss: 2.4375\n",
      "ep: 0, global_step: 77900, loss: 2.404296875\n",
      "ep: 0, global_step: 78000, loss: 2.15234375\n",
      "ep: 0, global_step: 78100, loss: 2.14453125\n",
      "ep: 0, global_step: 78200, loss: 2.10546875\n",
      "ep: 0, global_step: 78300, loss: 2.255859375\n",
      "ep: 0, global_step: 78400, loss: 1.8935546875\n",
      "ep: 0, global_step: 78500, loss: 1.794921875\n",
      "ep: 0, global_step: 78600, loss: 2.3671875\n",
      "ep: 0, global_step: 78700, loss: 2.267578125\n",
      "ep: 0, global_step: 78800, loss: 2.498046875\n",
      "ep: 0, global_step: 78900, loss: 2.56640625\n",
      "ep: 0, global_step: 79000, loss: 2.19921875\n",
      "ep: 0, global_step: 79100, loss: 1.9150390625\n",
      "ep: 0, global_step: 79200, loss: 2.275390625\n",
      "ep: 0, global_step: 79300, loss: 2.22265625\n",
      "ep: 0, global_step: 79400, loss: 2.2734375\n",
      "ep: 0, global_step: 79500, loss: 2.306640625\n",
      "ep: 0, global_step: 79600, loss: 2.1953125\n",
      "ep: 0, global_step: 79700, loss: 2.318359375\n",
      "ep: 0, global_step: 79800, loss: 2.0859375\n",
      "ep: 0, global_step: 79900, loss: 2.216796875\n",
      "ep: 0, global_step: 80000, loss: 2.52734375\n",
      "ep: 0, global_step: 80100, loss: 2.314453125\n",
      "ep: 0, global_step: 80200, loss: 2.634765625\n",
      "ep: 0, global_step: 80300, loss: 2.16015625\n",
      "ep: 0, global_step: 80400, loss: 2.091796875\n",
      "ep: 0, global_step: 80500, loss: 2.365234375\n",
      "ep: 0, global_step: 80600, loss: 1.9755859375\n",
      "ep: 0, global_step: 80700, loss: 2.22265625\n",
      "ep: 0, global_step: 80800, loss: 2.416015625\n",
      "ep: 0, global_step: 80900, loss: 2.26171875\n",
      "ep: 0, global_step: 81000, loss: 2.142578125\n",
      "ep: 0, global_step: 81100, loss: 2.609375\n",
      "ep: 0, global_step: 81200, loss: 2.396484375\n",
      "ep: 0, global_step: 81300, loss: 2.63671875\n",
      "ep: 0, global_step: 81400, loss: 2.150390625\n",
      "ep: 0, global_step: 81500, loss: 2.126953125\n",
      "ep: 0, global_step: 81600, loss: 1.7041015625\n",
      "ep: 0, global_step: 81700, loss: 2.44921875\n",
      "ep: 0, global_step: 81800, loss: 2.17578125\n",
      "ep: 0, global_step: 81900, loss: 2.5\n",
      "ep: 0, global_step: 82000, loss: 2.287109375\n",
      "ep: 0, global_step: 82100, loss: 2.259765625\n",
      "ep: 0, global_step: 82200, loss: 2.326171875\n",
      "ep: 0, global_step: 82300, loss: 2.1328125\n",
      "ep: 0, global_step: 82400, loss: 2.232421875\n",
      "ep: 0, global_step: 82500, loss: 2.349609375\n",
      "ep: 0, global_step: 82600, loss: 2.00390625\n",
      "ep: 0, global_step: 82700, loss: 2.224609375\n",
      "ep: 0, global_step: 82800, loss: 2.298828125\n",
      "ep: 0, global_step: 82900, loss: 2.0859375\n",
      "ep: 0, global_step: 83000, loss: 2.498046875\n",
      "ep: 0, global_step: 83100, loss: 2.4609375\n",
      "ep: 0, global_step: 83200, loss: 2.28125\n",
      "ep: 0, global_step: 83300, loss: 2.26171875\n",
      "ep: 0, global_step: 83400, loss: 2.302734375\n",
      "ep: 0, global_step: 83500, loss: 2.154296875\n",
      "ep: 0, global_step: 83600, loss: 2.162109375\n",
      "ep: 0, global_step: 83700, loss: 2.1640625\n",
      "ep: 0, global_step: 83800, loss: 2.736328125\n",
      "ep: 0, global_step: 83900, loss: 2.52734375\n",
      "ep: 0, global_step: 84000, loss: 2.53125\n",
      "ep: 0, global_step: 84100, loss: 1.94921875\n",
      "ep: 0, global_step: 84200, loss: 2.3359375\n",
      "ep: 0, global_step: 84300, loss: 2.2265625\n",
      "ep: 0, global_step: 84400, loss: 2.1171875\n",
      "ep: 0, global_step: 84500, loss: 2.107421875\n",
      "ep: 0, global_step: 84600, loss: 2.103515625\n",
      "ep: 0, global_step: 84700, loss: 2.43359375\n",
      "ep: 0, global_step: 84800, loss: 2.158203125\n",
      "ep: 0, global_step: 84900, loss: 1.994140625\n",
      "ep: 0, global_step: 85000, loss: 2.134765625\n",
      "ep: 0, global_step: 85100, loss: 2.04296875\n",
      "ep: 0, global_step: 85200, loss: 2.083984375\n",
      "ep: 0, global_step: 85300, loss: 2.611328125\n",
      "ep: 0, global_step: 85400, loss: 1.8369140625\n",
      "ep: 0, global_step: 85500, loss: 2.466796875\n",
      "ep: 0, global_step: 85600, loss: 2.40625\n",
      "ep: 0, global_step: 85700, loss: 2.169921875\n",
      "ep: 0, global_step: 85800, loss: 2.0625\n",
      "ep: 0, global_step: 85900, loss: 2.171875\n",
      "ep: 0, global_step: 86000, loss: 2.009765625\n",
      "ep: 0, global_step: 86100, loss: 2.001953125\n",
      "ep: 0, global_step: 86200, loss: 2.08203125\n",
      "ep: 0, global_step: 86300, loss: 2.513671875\n",
      "ep: 0, global_step: 86400, loss: 2.3203125\n",
      "ep: 0, global_step: 86500, loss: 2.47265625\n",
      "ep: 0, global_step: 86600, loss: 2.1171875\n",
      "ep: 0, global_step: 86700, loss: 2.478515625\n",
      "ep: 0, global_step: 86800, loss: 2.29296875\n",
      "ep: 0, global_step: 86900, loss: 2.42578125\n",
      "ep: 0, global_step: 87000, loss: 2.521484375\n",
      "ep: 0, global_step: 87100, loss: 2.296875\n",
      "ep: 0, global_step: 87200, loss: 2.1484375\n",
      "ep: 0, global_step: 87300, loss: 2.2578125\n",
      "ep: 0, global_step: 87400, loss: 2.37109375\n",
      "ep: 0, global_step: 87500, loss: 2.037109375\n",
      "ep: 0, global_step: 87600, loss: 2.115234375\n",
      "ep: 0, global_step: 87700, loss: 2.4453125\n",
      "ep: 0, global_step: 87800, loss: 2.271484375\n",
      "ep: 0, global_step: 87900, loss: 2.203125\n",
      "ep: 0, global_step: 88000, loss: 2.279296875\n",
      "ep: 0, global_step: 88100, loss: 2.53125\n",
      "ep: 0, global_step: 88200, loss: 2.337890625\n",
      "ep: 0, global_step: 88300, loss: 1.9033203125\n",
      "ep: 0, global_step: 88400, loss: 2.2265625\n",
      "ep: 0, global_step: 88500, loss: 2.447265625\n",
      "ep: 0, global_step: 88600, loss: 2.35546875\n",
      "ep: 0, global_step: 88700, loss: 2.419921875\n",
      "ep: 0, global_step: 88800, loss: 2.05078125\n",
      "ep: 0, global_step: 88900, loss: 2.486328125\n",
      "ep: 0, global_step: 89000, loss: 2.533203125\n",
      "ep: 0, global_step: 89100, loss: 1.91015625\n",
      "ep: 0, global_step: 89200, loss: 2.626953125\n",
      "ep: 0, global_step: 89300, loss: 2.33203125\n",
      "ep: 0, global_step: 89400, loss: 2.697265625\n",
      "ep: 0, global_step: 89500, loss: 2.3984375\n",
      "ep: 0, global_step: 89600, loss: 2.50390625\n",
      "ep: 0, global_step: 89700, loss: 2.552734375\n",
      "ep: 0, global_step: 89800, loss: 2.43359375\n",
      "ep: 0, global_step: 89900, loss: 2.287109375\n",
      "ep: 0, global_step: 90000, loss: 2.25\n",
      "ep: 0, global_step: 90100, loss: 2.1875\n",
      "ep: 0, global_step: 90200, loss: 2.638671875\n",
      "ep: 0, global_step: 90300, loss: 1.8623046875\n",
      "ep: 0, global_step: 90400, loss: 2.2265625\n",
      "ep: 0, global_step: 90500, loss: 2.607421875\n",
      "ep: 0, global_step: 90600, loss: 2.54296875\n",
      "ep: 0, global_step: 90700, loss: 2.08984375\n",
      "ep: 0, global_step: 90800, loss: 2.091796875\n",
      "ep: 0, global_step: 90900, loss: 2.12109375\n",
      "ep: 0, global_step: 91000, loss: 2.107421875\n",
      "ep: 0, global_step: 91100, loss: 2.33203125\n",
      "ep: 0, global_step: 91200, loss: 2.34375\n",
      "ep: 0, global_step: 91300, loss: 2.298828125\n",
      "ep: 0, global_step: 91400, loss: 2.392578125\n",
      "ep: 0, global_step: 91500, loss: 2.478515625\n",
      "ep: 0, global_step: 91600, loss: 2.19140625\n",
      "ep: 0, global_step: 91700, loss: 2.19140625\n",
      "ep: 0, global_step: 91800, loss: 2.38671875\n",
      "ep: 0, global_step: 91900, loss: 2.01171875\n",
      "ep: 0, global_step: 92000, loss: 2.330078125\n",
      "ep: 0, global_step: 92100, loss: 2.34375\n",
      "ep: 0, global_step: 92200, loss: 2.244140625\n",
      "ep: 0, global_step: 92300, loss: 2.388671875\n",
      "ep: 0, global_step: 92400, loss: 1.8818359375\n",
      "ep: 0, global_step: 92500, loss: 2.4921875\n",
      "ep: 0, global_step: 92600, loss: 2.392578125\n",
      "ep: 0, global_step: 92700, loss: 2.4453125\n",
      "ep: 0, global_step: 92800, loss: 2.333984375\n",
      "ep: 0, global_step: 92900, loss: 2.61328125\n",
      "ep: 0, global_step: 93000, loss: 2.244140625\n",
      "ep: 0, global_step: 93100, loss: 2.7421875\n",
      "ep: 0, global_step: 93200, loss: 2.44921875\n",
      "ep: 0, global_step: 93300, loss: 2.11328125\n",
      "ep: 0, global_step: 93400, loss: 2.1328125\n",
      "ep: 0, global_step: 93500, loss: 2.0\n",
      "ep: 0, global_step: 93600, loss: 2.4453125\n",
      "ep: 0, global_step: 93700, loss: 2.072265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, global_step: 93800, loss: 2.2578125\n",
      "ep: 0, global_step: 93900, loss: 2.484375\n",
      "ep: 0, global_step: 94000, loss: 2.12890625\n",
      "ep: 0, global_step: 94100, loss: 2.23046875\n",
      "ep: 0, global_step: 94200, loss: 2.521484375\n",
      "ep: 0, global_step: 94300, loss: 2.07421875\n",
      "ep: 0, global_step: 94400, loss: 2.416015625\n",
      "ep: 0, global_step: 94500, loss: 2.34375\n",
      "ep: 0, global_step: 94600, loss: 2.421875\n",
      "ep: 0, global_step: 94700, loss: 2.234375\n",
      "ep: 0, global_step: 94800, loss: 1.8310546875\n",
      "ep: 0, global_step: 94900, loss: 2.1640625\n",
      "ep: 0, global_step: 95000, loss: 2.330078125\n",
      "ep: 0, global_step: 95100, loss: 2.2578125\n",
      "ep: 0, global_step: 95200, loss: 2.294921875\n",
      "ep: 0, global_step: 95300, loss: 1.9833984375\n",
      "ep: 0, global_step: 95400, loss: 2.33984375\n",
      "ep: 0, global_step: 95500, loss: 2.1640625\n",
      "ep: 0, global_step: 95600, loss: 2.029296875\n",
      "ep: 0, global_step: 95700, loss: 2.35546875\n",
      "ep: 0, global_step: 95800, loss: 2.177734375\n",
      "ep: 0, global_step: 95900, loss: 2.25390625\n",
      "ep: 0, global_step: 96000, loss: 2.31640625\n",
      "ep: 0, global_step: 96100, loss: 2.03125\n",
      "ep: 0, global_step: 96200, loss: 2.380859375\n",
      "ep: 0, global_step: 96300, loss: 2.416015625\n",
      "ep: 0, global_step: 96400, loss: 1.7119140625\n",
      "ep: 0, global_step: 96500, loss: 2.140625\n",
      "ep: 0, global_step: 96600, loss: 2.517578125\n",
      "ep: 0, global_step: 96700, loss: 2.568359375\n",
      "ep: 0, global_step: 96800, loss: 2.251953125\n",
      "ep: 0, global_step: 96900, loss: 2.466796875\n",
      "ep: 0, global_step: 97000, loss: 2.66015625\n",
      "ep: 0, global_step: 97100, loss: 2.427734375\n",
      "ep: 0, global_step: 97200, loss: 2.5546875\n",
      "ep: 0, global_step: 97300, loss: 2.1875\n",
      "ep: 0, global_step: 97400, loss: 2.36328125\n",
      "ep: 0, global_step: 97500, loss: 2.23828125\n",
      "ep: 0, global_step: 97600, loss: 2.12109375\n",
      "ep: 0, global_step: 97700, loss: 1.99609375\n",
      "ep: 0, global_step: 97800, loss: 2.021484375\n",
      "ep: 0, global_step: 97900, loss: 2.501953125\n",
      "ep: 0, global_step: 98000, loss: 2.419921875\n",
      "ep: 0, global_step: 98100, loss: 2.4765625\n",
      "ep: 0, global_step: 98200, loss: 2.193359375\n",
      "ep: 0, global_step: 98300, loss: 2.177734375\n",
      "ep: 0, global_step: 98400, loss: 2.423828125\n",
      "ep: 0, global_step: 98500, loss: 1.908203125\n",
      "ep: 0, global_step: 98600, loss: 2.2890625\n",
      "ep: 0, global_step: 98700, loss: 2.48828125\n",
      "ep: 0, global_step: 98800, loss: 2.53125\n",
      "ep: 0, global_step: 98900, loss: 2.583984375\n",
      "ep: 0, global_step: 99000, loss: 2.28515625\n",
      "ep: 0, global_step: 99100, loss: 2.578125\n",
      "ep: 0, global_step: 99200, loss: 2.171875\n",
      "ep: 0, global_step: 99300, loss: 2.24609375\n",
      "ep: 0, global_step: 99400, loss: 2.228515625\n",
      "ep: 0, global_step: 99500, loss: 2.2734375\n",
      "ep: 0, global_step: 99600, loss: 2.39453125\n",
      "ep: 0, global_step: 99700, loss: 2.154296875\n",
      "ep: 0, global_step: 99800, loss: 2.0546875\n",
      "ep: 0, global_step: 99900, loss: 2.365234375\n",
      "ep: 0, global_step: 100000, loss: 2.224609375\n",
      "ep: 0, global_step: 100100, loss: 2.34765625\n",
      "ep: 0, global_step: 100200, loss: 2.357421875\n",
      "ep: 0, global_step: 100300, loss: 2.298828125\n",
      "ep: 0, global_step: 100400, loss: 2.091796875\n",
      "ep: 0, global_step: 100500, loss: 2.037109375\n",
      "ep: 0, global_step: 100600, loss: 2.53125\n",
      "ep: 0, global_step: 100700, loss: 2.12890625\n",
      "ep: 0, global_step: 100800, loss: 2.294921875\n",
      "ep: 0, global_step: 100900, loss: 2.31640625\n",
      "ep: 0, global_step: 101000, loss: 2.203125\n",
      "ep: 0, global_step: 101100, loss: 2.458984375\n",
      "ep: 0, global_step: 101200, loss: 2.349609375\n",
      "ep: 0, global_step: 101300, loss: 2.525390625\n",
      "ep: 0, global_step: 101400, loss: 2.09765625\n",
      "ep: 0, global_step: 101500, loss: 2.2265625\n",
      "ep: 0, global_step: 101600, loss: 2.171875\n",
      "ep: 0, global_step: 101700, loss: 2.556640625\n",
      "ep: 0, global_step: 101800, loss: 2.203125\n",
      "ep: 0, global_step: 101900, loss: 1.8154296875\n",
      "ep: 0, global_step: 102000, loss: 1.9404296875\n",
      "ep: 0, global_step: 102100, loss: 2.36328125\n",
      "ep: 0, global_step: 102200, loss: 2.47265625\n",
      "ep: 0, global_step: 102300, loss: 2.396484375\n",
      "ep: 0, global_step: 102400, loss: 2.349609375\n",
      "ep: 0, global_step: 102500, loss: 2.53515625\n",
      "ep: 0, global_step: 102600, loss: 2.083984375\n",
      "ep: 0, global_step: 102700, loss: 1.9423828125\n",
      "ep: 0, global_step: 102800, loss: 2.56640625\n",
      "ep: 0, global_step: 102900, loss: 2.439453125\n",
      "ep: 0, global_step: 103000, loss: 2.08984375\n",
      "ep: 0, global_step: 103100, loss: 2.251953125\n",
      "ep: 0, global_step: 103200, loss: 2.236328125\n",
      "ep: 0, global_step: 103300, loss: 2.451171875\n",
      "ep: 0, global_step: 103400, loss: 2.4296875\n",
      "ep: 0, global_step: 103500, loss: 2.361328125\n",
      "ep: 0, global_step: 103600, loss: 2.427734375\n",
      "ep: 0, global_step: 103700, loss: 2.177734375\n",
      "ep: 0, global_step: 103800, loss: 2.52734375\n",
      "ep: 0, global_step: 103900, loss: 2.228515625\n",
      "ep: 0, global_step: 104000, loss: 2.57421875\n",
      "ep: 0, global_step: 104100, loss: 1.8359375\n",
      "ep: 0, global_step: 104200, loss: 2.2890625\n",
      "ep: 0, global_step: 104300, loss: 1.943359375\n",
      "ep: 0, global_step: 104400, loss: 2.251953125\n",
      "ep: 0, global_step: 104500, loss: 2.58984375\n",
      "ep: 0, global_step: 104600, loss: 1.7880859375\n",
      "ep: 0, global_step: 104700, loss: 2.353515625\n",
      "ep: 0, global_step: 104800, loss: 2.380859375\n",
      "ep: 0, global_step: 104900, loss: 1.962890625\n",
      "ep: 0, global_step: 105000, loss: 2.529296875\n",
      "ep: 0, global_step: 105100, loss: 2.265625\n",
      "ep: 0, global_step: 105200, loss: 2.4765625\n",
      "ep: 0, global_step: 105300, loss: 2.29296875\n",
      "ep: 0, global_step: 105400, loss: 1.888671875\n",
      "ep: 0, global_step: 105500, loss: 2.455078125\n",
      "ep: 0, global_step: 105600, loss: 2.2421875\n",
      "ep: 0, global_step: 105700, loss: 2.0390625\n",
      "ep: 0, global_step: 105800, loss: 2.283203125\n",
      "ep: 0, global_step: 105900, loss: 2.6328125\n",
      "ep: 0, global_step: 106000, loss: 1.8935546875\n",
      "ep: 0, global_step: 106100, loss: 2.380859375\n",
      "ep: 0, global_step: 106200, loss: 2.1640625\n",
      "ep: 0, global_step: 106300, loss: 2.251953125\n",
      "ep: 0, global_step: 106400, loss: 2.208984375\n",
      "ep: 0, global_step: 106500, loss: 2.154296875\n",
      "ep: 0, global_step: 106600, loss: 1.7158203125\n",
      "ep: 0, global_step: 106700, loss: 2.30859375\n",
      "ep: 0, global_step: 106800, loss: 2.212890625\n",
      "ep: 0, global_step: 106900, loss: 2.240234375\n",
      "ep: 0, global_step: 107000, loss: 2.251953125\n",
      "ep: 0, global_step: 107100, loss: 1.9970703125\n",
      "ep: 0, global_step: 107200, loss: 2.091796875\n",
      "ep: 0, global_step: 107300, loss: 2.298828125\n",
      "ep: 0, global_step: 107400, loss: 2.349609375\n",
      "ep: 0, global_step: 107500, loss: 2.09765625\n",
      "ep: 0, global_step: 107600, loss: 2.009765625\n",
      "ep: 0, global_step: 107700, loss: 2.208984375\n",
      "ep: 0, global_step: 107800, loss: 2.275390625\n",
      "ep: 0, global_step: 107900, loss: 1.9541015625\n",
      "ep: 0, global_step: 108000, loss: 2.099609375\n",
      "ep: 0, global_step: 108100, loss: 2.380859375\n",
      "ep: 0, global_step: 108200, loss: 2.611328125\n",
      "ep: 0, global_step: 108300, loss: 2.328125\n",
      "ep: 0, global_step: 108400, loss: 2.51171875\n",
      "ep: 0, global_step: 108500, loss: 2.05078125\n",
      "ep: 0, global_step: 108600, loss: 2.037109375\n",
      "ep: 0, global_step: 108700, loss: 2.21875\n",
      "ep: 0, global_step: 108800, loss: 2.30078125\n",
      "ep: 0, global_step: 108900, loss: 2.12109375\n",
      "ep: 0, global_step: 109000, loss: 2.55078125\n",
      "ep: 0, global_step: 109100, loss: 1.9677734375\n",
      "ep: 0, global_step: 109200, loss: 2.337890625\n",
      "ep: 0, global_step: 109300, loss: 2.375\n",
      "ep: 0, global_step: 109400, loss: 2.4921875\n",
      "ep: 0, global_step: 109500, loss: 2.173828125\n",
      "ep: 0, global_step: 109600, loss: 2.060546875\n",
      "ep: 0, global_step: 109700, loss: 2.0390625\n",
      "ep: 0, global_step: 109800, loss: 2.556640625\n",
      "ep: 0, global_step: 109900, loss: 2.251953125\n",
      "ep: 0, global_step: 110000, loss: 2.091796875\n",
      "ep: 0, global_step: 110100, loss: 2.458984375\n",
      "ep: 0, global_step: 110200, loss: 1.5927734375\n",
      "ep: 0, global_step: 110300, loss: 2.158203125\n",
      "ep: 0, global_step: 110400, loss: 2.546875\n",
      "ep: 0, global_step: 110500, loss: 2.451171875\n",
      "ep: 0, global_step: 110600, loss: 2.08203125\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter(save_model_path)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-8, eps=1e-5, weight_decay=0.01)\n",
    "model = model.to(torch.device(device_str))\n",
    "def train(trainloader, epoch=1, log_step=100):\n",
    "    global_step = 0\n",
    "    lowest_loss = 0\n",
    "    save_step = 100000\n",
    "    for ep in range(epoch):\n",
    "        model.train()\n",
    "        for batch in trainloader:\n",
    "            # inp = batch['input_ids'][0]\n",
    "            # print(inp)\n",
    "            # print(tokenizer.decode(inp))\n",
    "            # tokenizer(tokenizer.decode(inp))\n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k: v.to(torch.device(device_str)) for k, v in batch.items()}\n",
    "            # with torch.autograd.detect_anomaly():\n",
    "            optimizer.zero_grad()\n",
    "            output = model(**batch)\n",
    "            # for name, param in model.named_parameters():\n",
    "                # if param.requires_grad:\n",
    "                #     # print(torch.isnan(param.grad).any())\n",
    "                #     # print('name:{} param grad:{} param requires_grad:{},params:{}'.format(name, param.grad, param.requires_grad,param))\n",
    "                #     print('name:{} param requires_grad:{}'.format(name, param.requires_grad))\n",
    "            loss = output.loss\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 20)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if global_step % log_step == 0:\n",
    "                print(f\"ep: {ep}, global_step: {global_step}, loss: {output.loss.item()}\")\n",
    "                if lowest_loss > output.loss.item():\n",
    "                    lowest_loss = output.loss.item()\n",
    "                    torch.save(model.state_dict(), save_model_path+'pth')\n",
    "                if  global_step % save_step == 0: \n",
    "                    torch.save(model.state_dict(), save_model_path+'pth')\n",
    "                writer.add_scalar('loss', output.loss.item(), global_step=global_step)\n",
    "            global_step += 1\n",
    "train(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
