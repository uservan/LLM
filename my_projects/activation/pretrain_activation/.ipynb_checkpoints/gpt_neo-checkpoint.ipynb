{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 04:30:02.647502: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-15 04:30:02.679483: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-15 04:30:03.283755: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM,\n",
    "                          GPTNeoForCausalLM, GPT2TokenizerFast,LlamaConfig)\n",
    "import os\n",
    "import random\n",
    "from typing import *\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer, BloomForCausalLM\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGPTNeoSelfAttention(nn.Module):\n",
    "    def __init__(self, config, device, is_linear):\n",
    "        super().__init__()\n",
    "\n",
    "        max_positions = config.max_position_embeddings\n",
    "        # bias = torch.tril(torch.ones((max_positions, max_positions), dtype=bool)).view(\n",
    "        #     1, 1, max_positions, max_positions\n",
    "        # )\n",
    "\n",
    "        # local causal self attention is a sliding window where each token can only attend to the previous\n",
    "        # window_size tokens. This is implemented by updating the causal mask such that for each token\n",
    "        # all other tokens are masked except the previous window_size tokens.\n",
    "        # if attention_type == \"local\":\n",
    "        #     bias = torch.bitwise_xor(bias, torch.tril(bias, -config.window_size))\n",
    "\n",
    "        # self.attn_dropout = nn.Dropout(float(config.attention_dropout))\n",
    "        self.resid_dropout = nn.Dropout(float(config.resid_dropout))\n",
    "\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        if self.head_dim * self.num_heads != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n",
    "                f\" {self.num_heads}).\"\n",
    "            )\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(in_features=self.embed_dim, out_features=self.embed_dim*2, bias=False, dtype=torch.float16),\n",
    "                                    nn.Linear(in_features=self.embed_dim*2, out_features=self.embed_dim, bias=False, dtype=torch.float16))\n",
    "        from torch.nn.init import xavier_uniform_\n",
    "        xavier_uniform_(self.linear[0].weight.data)\n",
    "        xavier_uniform_(self.linear[1].weight.data)\n",
    "        self.device = device\n",
    "        self.is_linear = is_linear\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        layer_past=None,\n",
    "        head_mask=None,\n",
    "        use_cache=False,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "        input_dtype = hidden_states.dtype\n",
    "        if input_dtype != torch.float16:\n",
    "            self.linear = self.linear.to(input_dtype)\n",
    "\n",
    "        key = torch.ones((bsz, q_len, self.num_heads, self.head_dim), dtype=input_dtype).to(\n",
    "            self.device).transpose(1, 2)\n",
    "        value = torch.ones((bsz, q_len, self.num_heads, self.head_dim), dtype=input_dtype).to(\n",
    "            self.device).transpose(1, 2)\n",
    "\n",
    "        attn_weights = torch.zeros((bsz, self.num_heads, q_len, q_len), dtype=input_dtype).to(self.device)\n",
    "\n",
    "        attn_output = self.linear(hidden_states)\n",
    "        p = self.linear.parameters()\n",
    "        if self.is_linear:\n",
    "            # attn_output = self.resid_dropout(attn_output)\n",
    "            pass\n",
    "        else:\n",
    "            attn_output = torch.ones_like(attn_output, dtype=input_dtype).to(self.device)\n",
    "\n",
    "        if layer_past is not None:\n",
    "            past_key = layer_past[0]\n",
    "            past_value = layer_past[1]\n",
    "            key = torch.cat((past_key, key), dim=-2)\n",
    "            value = torch.cat((past_value, value), dim=-2)\n",
    "\n",
    "        if use_cache is True:\n",
    "            present = (key, value)\n",
    "        else:\n",
    "            present = None\n",
    "\n",
    "        outputs = (attn_output, present)\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights,)\n",
    "\n",
    "        return outputs  # a, present, (attentions)\n",
    "\n",
    "class MyLlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config: LlamaConfig, device, is_linear):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.rope_theta = config.rope_theta\n",
    "        self.is_causal = True\n",
    "\n",
    "        # self.resid_dropout = nn.Dropout(float(config.resid_dropout))\n",
    "        self.linear = nn.Sequential(nn.Linear(in_features=2048, out_features=5632, bias=False, dtype=torch.float16),\n",
    "                                                      nn.Linear(in_features=5632, out_features=2048, bias=False, dtype=torch.float16))\n",
    "        self.device = device\n",
    "        self.is_linear =is_linear\n",
    "        if (self.head_dim * self.num_heads) != self.hidden_size:\n",
    "            raise ValueError(\n",
    "                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n",
    "                f\" and `num_heads`: {self.num_heads}).\"\n",
    "            )\n",
    "\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "        input_dtype = hidden_states.dtype\n",
    "        if input_dtype != torch.float16:\n",
    "            self.linear = self.linear.to(input_dtype)\n",
    "\n",
    "\n",
    "        key_states = torch.zeros((bsz, q_len, self.num_key_value_heads, self.head_dim), dtype=input_dtype).to(self.device).transpose(1, 2)\n",
    "        value_states = torch.zeros((bsz, q_len, self.num_key_value_heads, self.head_dim), dtype=input_dtype).to(self.device).transpose(1, 2)\n",
    "\n",
    "        past_key_value = (key_states, value_states) if use_cache else None\n",
    "\n",
    "        attn_weights = torch.zeros((bsz, self.num_heads, q_len, q_len), dtype=input_dtype).to(self.device)\n",
    "\n",
    "        attn_output = self.linear(hidden_states)\n",
    "        if not self.is_linear:\n",
    "            attn_output = torch.zeros_like(attn_output, dtype=input_dtype).to(self.device)\n",
    "\n",
    "        return attn_output, attn_weights, past_key_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_heatMap_sns(scores, save_path=None, title=None, cmap=None, y_ticks=None, x_ticks=None, show=None):\n",
    "    plt.subplots(figsize=(20, 20), dpi=200)\n",
    "    plt.rcParams['font.size'] = '10'\n",
    "    if cmap is None:\n",
    "        cmap = sns.color_palette(\"Reds\", as_cmap=True)\n",
    "    if x_ticks and y_ticks:\n",
    "        sns.heatmap(scores, cmap=cmap,  xticklabels=x_ticks, yticklabels=y_ticks)\n",
    "    else:\n",
    "        sns.heatmap(scores, cmap=cmap)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(os.path.join(save_path, f'{title}.png'), bbox_inches=\"tight\")\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def load_model(model_name: str, device='cuda', low_cpu_mem_usage=False, layers=[], train_layers=[], is_linear=False):\n",
    "    tokenizer: GPT2TokenizerFast = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    if model_name.find('gpt-neo') != -1:\n",
    "        model: GPTNeoForCausalLM = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)# .to(device)\n",
    "\n",
    "        for layer_id in layers:\n",
    "            model.transformer.h[layer_id].attn.attention = MyGPTNeoSelfAttention(model.config, device=device,\n",
    "                                                                                 is_linear=is_linear).to(device)\n",
    "            # model.model.layers[layer_id].self_attn = LlamaAttentionLinear(model.config , device).to(device)\n",
    "\n",
    "        MODEL_CONFIG = {\"n_heads\": model.config.num_attention_heads,\n",
    "                        \"n_layers\": model.config.num_hidden_layers,\n",
    "                        \"resid_dim\": model.config.hidden_size,\n",
    "                        \"name_or_path\": model.config.name_or_path,\n",
    "                        \"attn_hook_names\": [f'transformer.h.{layer}.attn.attention.attn_dropout' for layer in\n",
    "                                            range(model.config.num_hidden_layers)]\n",
    "                        }\n",
    "        # p = model.named_parameters()\n",
    "        if len(train_layers) > 0:\n",
    "            for k, params in model.named_parameters():\n",
    "                for layer_id in train_layers:\n",
    "                    if k.find(f'transformer.h.{layer_id}.attn.attention') == -1:\n",
    "                        params.requires_grad = False\n",
    "\n",
    "        return model, tokenizer, MODEL_CONFIG\n",
    "    if model_name.find('TinyLlama') != -1:\n",
    "        model: LlamaForCausalLM = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)#.to(device)\n",
    "\n",
    "        for layer_id in layers:\n",
    "            # model.transformer.h[layer_id].attn.attention = MyGPTNeoSelfAttention(model.config, device=device,is_linear=is_linear).to(device)\n",
    "            model.model.layers[layer_id].self_attn = MyLlamaAttention(model.config, device, is_linear=is_linear).to(\n",
    "                device)\n",
    "\n",
    "        MODEL_CONFIG = {\"n_heads\": model.config.num_attention_heads,\n",
    "                        \"n_layers\": model.config.num_hidden_layers,\n",
    "                        \"resid_dim\": model.config.hidden_size,\n",
    "                        \"name_or_path\": model.config.name_or_path,\n",
    "                        \"attn_hook_names\": [f'model.layers.{layer}.self_attn.attn_dropout' for layer in\n",
    "                                            range(model.config.num_hidden_layers)]\n",
    "                        }\n",
    "        if len(train_layers) > 0:\n",
    "            for k, params in model.named_parameters():\n",
    "                for layer_id in train_layers:\n",
    "                    if k.find(str(layer_id)) == -1:\n",
    "                        params.requires_grad = False\n",
    "\n",
    "        return model, tokenizer, MODEL_CONFIG\n",
    "\n",
    "\n",
    "def draw_attention(model, tokenizer, MODEL_CONFIG, device, prompt, check_token_id, save_path, title):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    output_and_cache = model(**inputs, output_hidden_states=True, output_attentions=True)\n",
    "    ground_attentions = torch.cat(output_and_cache.attentions, dim=0).detach().cpu().numpy()\n",
    "    ground_attentions = ground_attentions[:, :, check_token_id, :]\n",
    "\n",
    "    x_ticks = [f\"layer{i + 1}\" for i in range(MODEL_CONFIG['n_layers'])]\n",
    "    encoded_line = tokenizer.encode(prompt)\n",
    "    codes = tokenizer.convert_ids_to_tokens(encoded_line)\n",
    "    y_ticks = [f\"head{i_head}-{c}\" for i_head in range(MODEL_CONFIG['n_heads']) for i, c in enumerate(codes)]\n",
    "    plt_heatMap_sns(ground_attentions.reshape(ground_attentions.shape[0], -1).T,\n",
    "                    title=title, x_ticks=x_ticks, y_ticks=y_ticks\n",
    "                    , show=True, save_path=save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [22]  # [23]\n",
    "train_layers = [22]\n",
    "is_linear = False\n",
    "save_model_path = f'./results/gpt_neo_{layers}_{is_linear}_{train_layers}'\n",
    "\n",
    "device_str = 'cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model_name = 'EleutherAI/gpt-neo-1.3B'  # 'EleutherAI/gpt-neo-1.3B' # 'TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T' # 'EleutherAI/gpt-neo-125m'\n",
    "model, tokenizer, MODEL_CONFIG = load_model(model_name, device=device_str, layers=layers, train_layers=train_layers,\n",
    "                                            is_linear=is_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "tokenizer\n",
    "# MODEL_CONFIG\n",
    "print(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4f9c125e134069bc1ce1b4fdc8c01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6073fe10c2f54d068c7cea69889771e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1770097 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " # dataset = load_dataset('cerebras/SlimPajama-627B',split='train[:100]') # TinyLlama\n",
    "dataset = load_dataset('monology/pile-uncopyrighted', split='train[:1%]')  # gpt-neo\n",
    "\n",
    "def process_func(examples):\n",
    "    contents = [e + tokenizer.eos_token for e in examples[\"text\"]]\n",
    "    return tokenizer(contents, max_length=128, truncation=True)\n",
    "tokenized_ds = dataset.map(process_func, batched=True, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([39112,   533,  1430,    26, 13771,  5197,   286, 15077,  5010,   973,\n",
      "          287, 16217,   418,  7211,  3314,  9102,   438, 16045,  7708,    13,\n",
      "         8125,  3896,    13,   198,  1212,  2457,  3896,   716,  2412,   262,\n",
      "         6647,   284,  2148, 13771,  5197,   329, 15077,  5010,   973,   287,\n",
      "        16217,   418,  7211,  3314,  9102, 30760,   284,   281,  1981,   508,\n",
      "        11583,   281,  1618, 23319,   329,   543, 13771,  6074,   318,   925,\n",
      "           13,   770,  3896, 12497,   262, 28547,   286,  2665, 45278,     7,\n",
      "           82,  5769,    17,  5769,    41,     8,   286,   262,  5483,  4765,\n",
      "         2191,   326,  3769, 13771,  5197,   329, 15077,  5010,   973,   287,\n",
      "        16217,   418,  7211,  3314,  9102,   329,   257,  2278,   286,   510,\n",
      "          284,   352,   614,   422,   262,  3128,   286, 17655,   422,   281,\n",
      "          287, 26029,  4436,  2652,  1141,   543,   262, 13771,    12, 32111,\n",
      "         1618,   393, 10712, 23319,   373,  6157,    13,   770])\n",
      "Medicare program; Medicare coverage of prescription drugs used in immunosuppressive therapy--HCFA. Final rule.\n",
      "This final rule amends the regulations to provide Medicare coverage for prescription drugs used in immunosuppressive therapy furnished to an individual who receives an organ transplant for which Medicare payment is made. This rule reflects the enactment of section 1861(s)(2)(J) of the Social Security Act that provides Medicare coverage for prescription drugs used in immunosuppressive therapy for a period of up to 1 year from the date of discharge from an inpatient hospital stay during which the Medicare-covered organ or tissue transplant was performed. This\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [39112, 533, 1430, 26, 13771, 5197, 286, 15077, 5010, 973, 287, 16217, 418, 7211, 3314, 9102, 438, 16045, 7708, 13, 8125, 3896, 13, 198, 1212, 2457, 3896, 716, 2412, 262, 6647, 284, 2148, 13771, 5197, 329, 15077, 5010, 973, 287, 16217, 418, 7211, 3314, 9102, 30760, 284, 281, 1981, 508, 11583, 281, 1618, 23319, 329, 543, 13771, 6074, 318, 925, 13, 770, 3896, 12497, 262, 28547, 286, 2665, 45278, 7, 82, 5769, 17, 5769, 41, 8, 286, 262, 5483, 4765, 2191, 326, 3769, 13771, 5197, 329, 15077, 5010, 973, 287, 16217, 418, 7211, 3314, 9102, 329, 257, 2278, 286, 510, 284, 352, 614, 422, 262, 3128, 286, 17655, 422, 281, 287, 26029, 4436, 2652, 1141, 543, 262, 13771, 12, 32111, 1618, 393, 10712, 23319, 373, 6157, 13, 770], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(len(tokenized_ds['input_ids'][2]))\n",
    "# print(len(tokenized_ds['input_ids'][0]))\n",
    "dl = DataLoader(tokenized_ds, batch_size=1, collate_fn=DataCollatorForLanguageModeling(tokenizer, mlm=False), shuffle=True)\n",
    "inp = next(enumerate(dl))[1]['input_ids'][0]\n",
    "print(inp)\n",
    "print(tokenizer.decode(inp))\n",
    "tokenizer(tokenizer.decode(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m                     writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, output\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem(), global_step\u001b[38;5;241m=\u001b[39mglobal_step)\n\u001b[1;32m     38\u001b[0m                 global_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 39\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(trainloader, epoch, log_step)\u001b[0m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     21\u001b[0m output \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m---> 22\u001b[0m \u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maccum_steps\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# torch.nn.utils.clip_grad_norm_(model.parameters(), 20)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m accum_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(trainloader):\n",
      "File \u001b[0;32m/scr1/wyang107/env/conda2/envs/LLM/lib/python3.8/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scr1/wyang107/env/conda2/envs/LLM/lib/python3.8/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter(save_model_path)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-8, eps=1e-5, weight_decay=0.01)\n",
    "model = model.to(torch.device(device_str))\n",
    "def train(trainloader, epoch=1, log_step=100):\n",
    "    global_step = 0\n",
    "    lowest_loss = 10\n",
    "    accum_steps = 8\n",
    "    for ep in range(epoch):\n",
    "        model.train()\n",
    "        for idx, batch in enumerate(trainloader):\n",
    "            # inp = batch['input_ids'][0]\n",
    "            # print(inp)\n",
    "            # print(tokenizer.decode(inp))\n",
    "            # tokenizer(tokenizer.decode(inp))\n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k: v.to(torch.device(device_str)) for k, v in batch.items()}\n",
    "            # with torch.autograd.detect_anomaly():\n",
    "            optimizer.zero_grad()\n",
    "            output = model(**batch)\n",
    "            loss = output.loss\n",
    "            (loss/ accum_steps).backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 20)\n",
    "            if (idx+1) % accum_steps == 0 or (idx+1) == len(trainloader):\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                # for name, param in model.named_parameters():\n",
    "                #     if param.requires_grad:\n",
    "                #         print(torch.isnan(param.grad).any())\n",
    "                #         print('name:{} param grad:{} param requires_grad:{},params:{}'.format(name, param.grad,\n",
    "                #                                                                    param.requires_grad,param))\n",
    "                if global_step % log_step == 0:\n",
    "                    print(f\"ep: {ep}, global_step: {global_step}, loss: {output.loss.item()}\")\n",
    "                    if lowest_loss > output.loss.item():\n",
    "                        lowest_loss = output.loss.item()\n",
    "                        torch.save(model.state_dict(), save_model_path+'pth')\n",
    "                    writer.add_scalar('loss', output.loss.item(), global_step=global_step)\n",
    "                global_step += 1\n",
    "train(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
